{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm-alstm-dtml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CBMER2hsMnf5441MRMXHzyhEmTp2s0qX",
      "authorship_tag": "ABX9TyMxjJugG40OGcUldUobexQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andy9705/stock_project/blob/main/lstm_alstm_dtml_acl18%EA%B2%B0%EA%B3%BC(0727).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFkSVnjzTC0E",
        "outputId": "ee77db88-d4b8-47c6-eac0-c8dcd6c03d93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/jolproject/Adv-ALSTM\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/jolproject/Adv-ALSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXetBnELVvoB",
        "outputId": "ac8f2634-deca-42d7-b8be-a5302cf1a84c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/         hyperparameters  pred_lstm.py  README.md     \u001b[01;34mtmp\u001b[0m/\n",
            "evaluator.py  load.py          \u001b[01;34m__pycache__\u001b[0m/  \u001b[01;34msaved_model\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TC0HwrBi0Iip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fulifeng/Adv-ALSTM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUJ3RW_WUGSc",
        "outputId": "3753e5c6-3dcc-42cf-b587-a6ca136335db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Adv-ALSTM'...\n",
            "remote: Enumerating objects: 357, done.\u001b[K\n",
            "remote: Total 357 (delta 0), reused 0 (delta 0), pack-reused 357\u001b[K\n",
            "Receiving objects: 100% (357/357), 15.80 MiB | 7.67 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Checking out files: 100% (314/314), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/jolproject/Adv-ALSTM/')"
      ],
      "metadata": {
        "id": "3hiaVE_jTx8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def load_cla_data(data_path, tra_date, val_date, tes_date, seq=2,\n",
        "                  date_format='%Y-%m-%d'):\n",
        "    fnames = [fname for fname in os.listdir(data_path) if\n",
        "              os.path.isfile(os.path.join(data_path, fname))]\n",
        "    print(len(fnames), ' tickers selected')\n",
        "    print(fnames)\n",
        "    data_EOD = []\n",
        "    for index, fname in enumerate(fnames):\n",
        "        #print(fname)\n",
        "        single_EOD = np.genfromtxt(\n",
        "            os.path.join(data_path, fname), dtype=float, delimiter=',',\n",
        "            skip_header=False\n",
        "        )\n",
        "        #print('data shape:', single_EOD.shape)\n",
        "        data_EOD.append(single_EOD)\n",
        "    print(len(data_EOD)) #50\n",
        "    print(len(data_EOD[0])) #2518\n",
        "    print(len(data_EOD[0][0])) #13\n",
        "\n",
        "\n",
        "    fea_dim = data_EOD[0].shape[1] - 2\n",
        "\n",
        "    trading_dates = np.genfromtxt(\n",
        "        os.path.join(data_path, '..', 'trading_dates.csv'), dtype=str,\n",
        "        delimiter=',', skip_header=False\n",
        "    )\n",
        "    print(len(trading_dates), 'trading dates:')\n",
        "\n",
        "    # transform the trading dates into a dictionary with index, at the same\n",
        "    # time, transform the indices into a dictionary with weekdays\n",
        "    dates_index = {}\n",
        "    # indices_weekday = {}\n",
        "    data_wd = np.zeros([len(trading_dates), 5], dtype=float)\n",
        "    wd_encodings = np.identity(5, dtype=float)\n",
        "    for index, date in enumerate(trading_dates):\n",
        "        dates_index[date] = index\n",
        "        # indices_weekday[index] = datetime.strptime(date, date_format).weekday()\n",
        "        data_wd[index] = wd_encodings[datetime.strptime(date, date_format).weekday()]\n",
        "\n",
        "    tra_ind = dates_index[tra_date]\n",
        "    val_ind = dates_index[val_date]\n",
        "    tes_ind = dates_index[tes_date]\n",
        "    print(tra_ind, val_ind, tes_ind) #0 2014 2266\n",
        "\n",
        "    # count training, validation, and testing instances\n",
        "    tra_num = 0\n",
        "    val_num = 0\n",
        "    tes_num = 0\n",
        "    # training\n",
        "    temp=[]\n",
        "    for date_ind in range(tra_ind, val_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        \n",
        "        for tic_ind in range(len(fnames)):\n",
        "            #print(abs(data_EOD[tic_ind][date_ind][-2]))\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tra_num += 1\n",
        "            else:\n",
        "\n",
        "              temp.append([date_ind,tic_ind])\n",
        "    print(temp)\n",
        "    print(len(temp))\n",
        "    print(tra_num, ' training instances')\n",
        "\n",
        "    # validation\n",
        "    for date_ind in range(val_ind, tes_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                val_num += 1\n",
        "    print(val_num, ' validation instances')\n",
        "\n",
        "    # testing\n",
        "    for date_ind in range(tes_ind, len(trading_dates)):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tes_num += 1\n",
        "    print(tes_num, ' testing instances')\n",
        "\n",
        "    # generate training, validation, and testing instances\n",
        "    # training\n",
        "    tra_pv = np.zeros([tra_num, seq, fea_dim], dtype=float)\n",
        "    tra_wd = np.zeros([tra_num, seq, 5], dtype=float)\n",
        "    tra_gt = np.zeros([tra_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(tra_ind, val_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tra_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, : -2]\n",
        "                tra_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                tra_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2]  #ground truth\n",
        "                # print(data_EOD[tic_ind][date_ind - seq: date_ind, : -2])\n",
        "                # print(data_wd[date_ind - seq: date_ind, :])\n",
        "                # print((data_EOD[tic_ind][date_ind][-2] + 1) / 2)\n",
        "                # print(date_ind)\n",
        "                \n",
        "                ins_ind += 1\n",
        "\n",
        "    # validation\n",
        "    val_pv = np.zeros([val_num, seq, fea_dim], dtype=float)\n",
        "    val_wd = np.zeros([val_num, seq, 5], dtype=float)\n",
        "    val_gt = np.zeros([val_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(val_ind, tes_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                val_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :-2]\n",
        "                val_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                val_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2] \n",
        "                ins_ind += 1\n",
        "\n",
        "    # testing\n",
        "    tes_pv = np.zeros([tes_num, seq, fea_dim], dtype=float)\n",
        "    tes_wd = np.zeros([tes_num, seq, 5], dtype=float)\n",
        "    tes_gt = np.zeros([tes_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(tes_ind, len(trading_dates)):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tes_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :-2]\n",
        "                # # for the momentum indicator\n",
        "                # tes_pv[ins_ind, -1, -1] = data_EOD[tic_ind][date_ind - 1, -1] - data_EOD[tic_ind][date_ind - 11, -1]\n",
        "                tes_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                tes_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2] \n",
        "                ins_ind += 1\n",
        "    return tra_pv, tra_wd, tra_gt, val_pv, val_wd, val_gt, tes_pv, tes_wd, tes_gt\n",
        "\n",
        "\n",
        "def load_globalmarket_data(data_path, tra_date, val_date, tes_date, seq=2,\n",
        "                  date_format='%Y-%m-%d'):\n",
        "    fnames = [fname for fname in os.listdir(data_path) if\n",
        "              os.path.isfile(os.path.join(data_path, fname))]\n",
        "    print(len(fnames), ' tickers selected')\n",
        "    print(fnames)\n",
        "    data_EOD = []\n",
        "    for index, fname in enumerate(fnames):\n",
        "        #print(fname)\n",
        "        single_EOD = np.genfromtxt(\n",
        "            os.path.join(data_path, fname), dtype=float, delimiter=',',\n",
        "            skip_header=False\n",
        "        )\n",
        "        #print('data shape:', single_EOD.shape)\n",
        "        data_EOD.append(single_EOD)\n",
        "    print(len(data_EOD)) #50\n",
        "    print(len(data_EOD[0])) #2518\n",
        "    print(len(data_EOD[0][0])) #11\n",
        "\n",
        "\n",
        "    fea_dim = data_EOD[0].shape[1] \n",
        "\n",
        "    trading_dates = np.genfromtxt(\n",
        "        os.path.join(data_path, '..', 'trading_dates.csv'), dtype=str,\n",
        "        delimiter=',', skip_header=False\n",
        "    )\n",
        "    print(len(trading_dates), 'trading dates:')\n",
        "\n",
        "    # transform the trading dates into a dictionary with index, at the same\n",
        "    # time, transform the indices into a dictionary with weekdays\n",
        "    dates_index = {}\n",
        "    # indices_weekday = {}\n",
        "    data_wd = np.zeros([len(trading_dates), 5], dtype=float)\n",
        "    wd_encodings = np.identity(5, dtype=float)\n",
        "    for index, date in enumerate(trading_dates):\n",
        "        dates_index[date] = index\n",
        "        # indices_weekday[index] = datetime.strptime(date, date_format).weekday()\n",
        "        data_wd[index] = wd_encodings[datetime.strptime(date, date_format).weekday()]\n",
        "\n",
        "    tra_ind = dates_index[tra_date]\n",
        "    val_ind = dates_index[val_date]\n",
        "    tes_ind = dates_index[tes_date]\n",
        "    print(tra_ind, val_ind, tes_ind) #0 2014 2266\n",
        "\n",
        "    # count training, validation, and testing instances\n",
        "    tra_num = 0\n",
        "    val_num = 0\n",
        "    tes_num = 0\n",
        "    # training\n",
        "    \n",
        "    \n",
        "    for date_ind in range(tra_ind, val_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        \n",
        "        for tic_ind in range(len(fnames)):\n",
        "            #print(abs(data_EOD[tic_ind][date_ind][-2]))\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tra_num += 1\n",
        "                \n",
        "    print(tra_num, ' training instances')\n",
        "\n",
        "    # validation\n",
        "    for date_ind in range(val_ind, tes_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                val_num += 1\n",
        "    print(val_num, ' validation instances')\n",
        "\n",
        "    # testing\n",
        "    for date_ind in range(tes_ind, len(trading_dates)):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tes_num += 1\n",
        "    print(tes_num, ' testing instances')\n",
        "\n",
        "    # generate training, validation, and testing instances\n",
        "    # training\n",
        "    tra_pv = np.zeros([tra_num, seq, fea_dim], dtype=float)\n",
        "    tra_wd = np.zeros([tra_num, seq, 5], dtype=float)\n",
        "    tra_gt = np.zeros([tra_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(tra_ind, val_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            \n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tra_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, : ]\n",
        "                tra_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                tra_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2]  #ground truth\n",
        "                # print(data_EOD[tic_ind][date_ind - seq: date_ind, : -2])\n",
        "                # print(data_wd[date_ind - seq: date_ind, :])\n",
        "                # print((data_EOD[tic_ind][date_ind][-2] + 1) / 2)\n",
        "                # print(date_ind)\n",
        "                \n",
        "                ins_ind += 1\n",
        "\n",
        "    # validation\n",
        "    val_pv = np.zeros([val_num, seq, fea_dim], dtype=float)\n",
        "    val_wd = np.zeros([val_num, seq, 5], dtype=float)\n",
        "    val_gt = np.zeros([val_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(val_ind, tes_ind):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                val_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :]\n",
        "                val_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                val_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2] \n",
        "                ins_ind += 1\n",
        "\n",
        "    # testing\n",
        "    tes_pv = np.zeros([tes_num, seq, fea_dim], dtype=float)\n",
        "    tes_wd = np.zeros([tes_num, seq, 5], dtype=float)\n",
        "    tes_gt = np.zeros([tes_num, 1], dtype=float)\n",
        "    ins_ind = 0\n",
        "    for date_ind in range(tes_ind, len(trading_dates)):\n",
        "        # filter out instances without length enough history\n",
        "        if date_ind < seq:\n",
        "            continue\n",
        "        for tic_ind in range(len(fnames)):\n",
        "            if data_EOD[tic_ind][date_ind - seq: date_ind, :].min() > -123320:\n",
        "                tes_pv[ins_ind] = data_EOD[tic_ind][date_ind - seq: date_ind, :]\n",
        "                # # for the momentum indicator\n",
        "                # tes_pv[ins_ind, -1, -1] = data_EOD[tic_ind][date_ind - 1, -1] - data_EOD[tic_ind][date_ind - 11, -1]\n",
        "                tes_wd[ins_ind] = data_wd[date_ind - seq: date_ind, :]\n",
        "                tes_gt[ins_ind, 0] = data_EOD[tic_ind][date_ind][-2] \n",
        "                ins_ind += 1\n",
        "    return tra_pv, tra_wd, tra_gt, val_pv, val_wd, val_gt, tes_pv, tes_wd, tes_gt"
      ],
      "metadata": {
        "id": "FYYdkotKVGUT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from load import load_cla_data\n",
        "\n",
        "\n",
        "# if 'acl18' in args.path:\n",
        "#         tra_date = '2014-01-02'\n",
        "#         val_date = '2015-08-03'\n",
        "#         tes_date = '2015-10-01'\n",
        "#     elif 'kdd17' in args.path:\n",
        "#         tra_date = '2007-01-03'\n",
        "#         val_date = '2015-01-02'\n",
        "#         tes_date = '2016-01-04'\n",
        "\n",
        "# tra_date = '2007-01-03'\n",
        "# val_date = '2015-01-02'\n",
        "# tes_date = '2016-01-04'\n",
        "\n",
        "tra_date = '2014-01-02'\n",
        "val_date = '2015-08-03'\n",
        "tes_date = '2015-10-01'\n",
        "\n",
        "tra_pv, tra_wd,tra_gt, \\\n",
        "val_pv, val_wd, val_gt, \\\n",
        "tes_pv, tes_wd, tes_gt = load_cla_data(\n",
        "    \"./data/stocknet-dataset/price/ourpped/\",  #ACL18 ./data/stocknet-dataset/price/ourpped/, KDD ./data/kdd17/ourpped/\n",
        "    tra_date, val_date, tes_date, seq=10  ##10, 15 window size\n",
        ")\n",
        "fea_dim = tra_pv.shape[2]\n",
        "\n",
        "globaltra_pv, globaltra_wd,globaltra_gt, \\\n",
        "globalval_pv, globalval_wd, globalval_gt, \\\n",
        "globaltes_pv, globaltes_wd, globaltes_gt = load_globalmarket_data(\n",
        "    \"./data/stocknet-dataset/price/globalmarket/\",  #ACL18 ./data/stocknet-dataset/price/ourpped, KDD ./data/kdd17/ourpped/\n",
        "    tra_date, val_date, tes_date, seq=10  ##10, 15 window size\n",
        ")\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjauUi-pZZXG",
        "outputId": "e91f5651-373b-4916-ed19-adb5b7f28366"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85  tickers selected\n",
            "['AAPL.csv', 'ABBV.csv', 'AEP.csv', 'ABB.csv', 'AMGN.csv', 'AMZN.csv', 'BA.csv', 'BAC.csv', 'BCH.csv', 'BBL.csv', 'BHP.csv', 'BRK-A.csv', 'BP.csv', 'BSAC.csv', 'BUD.csv', 'C.csv', 'CAT.csv', 'CELG.csv', 'CHL.csv', 'CHTR.csv', 'CMCSA.csv', 'CODI.csv', 'CVX.csv', 'CSCO.csv', 'D.csv', 'DHR.csv', 'DIS.csv', 'EXC.csv', 'FB.csv', 'GD.csv', 'DUK.csv', 'GE.csv', 'GOOG.csv', 'HSBC.csv', 'HRG.csv', 'HON.csv', 'HD.csv', 'IEP.csv', 'JPM.csv', 'JNJ.csv', 'INTC.csv', 'KO.csv', 'LMT.csv', 'MMM.csv', 'MA.csv', 'MCD.csv', 'MDT.csv', 'MO.csv', 'MSFT.csv', 'MRK.csv', 'NEE.csv', 'NGG.csv', 'NVS.csv', 'ORCL.csv', 'PCG.csv', 'PEP.csv', 'PCLN.csv', 'PFE.csv', 'PG.csv', 'PICO.csv', 'PTR.csv', 'PM.csv', 'PPL.csv', 'RDS-B.csv', 'SNP.csv', 'SLB.csv', 'REX.csv', 'SNY.csv', 'SO.csv', 'T.csv', 'SRE.csv', 'TM.csv', 'SPLP.csv', 'TOT.csv', 'UN.csv', 'UNH.csv', 'UL.csv', 'TSM.csv', 'UPS.csv', 'UTX.csv', 'WFC.csv', 'VZ.csv', 'V.csv', 'XOM.csv', 'WMT.csv']\n",
            "85\n",
            "652\n",
            "13\n",
            "652 trading dates:\n",
            "148 546 588\n",
            "[]\n",
            "0\n",
            "33830  training instances\n",
            "3570  validation instances\n",
            "5440  testing instances\n",
            "1  tickers selected\n",
            "['sp500_acl18.csv']\n",
            "1\n",
            "652\n",
            "11\n",
            "652 trading dates:\n",
            "148 546 588\n",
            "398  training instances\n",
            "42  validation instances\n",
            "64  testing instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(globaltra_pv,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aeoc-3dnJraa",
        "outputId": "25587780-59f3-4e0c-dfad-45e77a0c35a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.03624 ,  1.92064 , -0.525452,  0.895792,  0.895792,  0.978439,\n",
              "          0.869724, -0.945822, -1.990395, -2.686717, -5.082441],\n",
              "        [ 0.271055,  0.560177, -0.758948,  0.27179 ,  0.27179 ,  0.05421 ,\n",
              "          0.914347, -0.88664 , -1.998555, -2.682327, -4.737381],\n",
              "        [-0.072439,  0.452735, -1.032233, -0.21684 , -0.21684 ,  0.068814,\n",
              "          1.291197, -0.231802, -1.568274, -2.218039, -3.889293],\n",
              "        [-1.122989,  0.39216 , -1.693398,  1.59362 ,  1.59362 , -1.408196,\n",
              "         -0.19964 , -1.292926, -2.757573, -3.546521, -4.761137],\n",
              "        [ 0.682106,  0.682106, -0.125651, -0.695185, -0.695185, -0.355411,\n",
              "          0.217197, -0.268054, -1.763596, -2.647279, -3.485907],\n",
              "        [ 0.017934,  0.50224 , -0.44843 ,  0.071802,  0.071802, -0.22601 ,\n",
              "         -0.130942, -0.171002, -1.512108, -2.425112, -3.091181],\n",
              "        [-0.321658,  0.035742, -0.607577,  0.37668 ,  0.37668 , -0.378841,\n",
              "         -0.716582, -0.262091, -1.583272, -2.543244, -3.196925],\n",
              "        [-0.089364,  0.268091, -0.196606, -0.017866, -0.017866, -0.100092,\n",
              "         -0.668456, -0.053622, -1.175159, -2.302058, -2.928212],\n",
              "        [ 1.193059,  1.373829, -0.090383, -1.126007, -1.126007,  0.755603,\n",
              "          0.368763,  1.056881,  0.263015, -0.958062, -1.696192],\n",
              "        [ 0.638218,  1.13056 , -0.656455, -0.867679, -0.867679,  1.320204,\n",
              "          1.272793,  1.644784,  1.315645,  0.099927, -0.699003]]),\n",
              " array([-123321.]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd \n",
        "from pylab import mpl, plt\n",
        "plt.style.use('seaborn')\n",
        "mpl.rcParams['font.family'] = 'serif'\n",
        "%matplotlib inline\n",
        "\n",
        "from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09bCOxPN769t",
        "outputId": "c0ae816b-b667-444e-ec93-75ad610db167"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "x_train = torch.from_numpy(tra_pv).type(torch.Tensor).to(device)\n",
        "x_test = torch.from_numpy(tes_pv).type(torch.Tensor).to(device)\n",
        "y_train = torch.from_numpy(tra_gt).type(torch.Tensor).to(device)\n",
        "y_test = torch.from_numpy(tes_gt).type(torch.Tensor).to(device)\n",
        "x_val = torch.from_numpy(val_pv).type(torch.Tensor).to(device)\n",
        "y_val = torch.from_numpy(val_gt).type(torch.Tensor).to(device)\n",
        "\n",
        "global_train = torch.from_numpy(globaltra_pv).type(torch.Tensor).to(device)\n",
        "global_val = torch.from_numpy(globalval_pv).type(torch.Tensor).to(device)\n",
        "global_test = torch.from_numpy(globaltes_pv).type(torch.Tensor).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD55Ft197umz",
        "outputId": "24d3c8c9-2060-464b-f4ed-f0b155869489"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.size(),x_train.size(),global_train.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAWiWaMA8VQq",
        "outputId": "893f3233-d889-4690-83ba-da0ffdfcab0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([33830, 1]),\n",
              " torch.Size([33830, 10, 11]),\n",
              " torch.Size([398, 10, 11]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(y_true, y_prob):\n",
        "    \n",
        "    y_prob = y_prob > 0.0\n",
        "    \n",
        "    return (y_true == y_prob).sum().item() / y_true.size(0)"
      ],
      "metadata": {
        "id": "63c14ellJNsp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "#####################\n",
        "input_dim = 11\n",
        "hidden_dim = 64\n",
        "num_layers = 1 \n",
        "output_dim = 1\n",
        "\n",
        "\n",
        "# Here we define our model as a class\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        #feature transformerlayer\n",
        "        self.feature_transformer=nn.Linear(input_dim,hidden_dim)\n",
        "        self.tanh_layer=nn.Tanh()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x=self.feature_transformer(x)\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x=self.tanh_layer(x)\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        #print(out.shape)  #98750, 10 ,32\n",
        "        #input()\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 32, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states!\n",
        "        #print(out[:, -1, :])\n",
        "        #print(out[:, -1, :].shape) #98750, 32\n",
        "        #input() \n",
        "        out = self.fc(out[:, -1, :])\n",
        "        #print(out)\n",
        "        #print(out.shape)\n",
        "        #input() \n",
        "        # out.size() --> 100, 10\n",
        "        return out\n",
        "    \n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)\n",
        "print(len(list(model.parameters())))\n",
        "for i in range(len(list(model.parameters()))):\n",
        "    print(list(model.parameters())[i].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYP20v7u9GGK",
        "outputId": "375877a4-715c-4955-f3ea-7023a84ed23c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (feature_transformer): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (tanh_layer): Tanh()\n",
            "  (lstm): LSTM(64, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "8\n",
            "torch.Size([64, 11])\n",
            "torch.Size([64])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "#####################\n",
        "num_epochs = 200\n",
        "hist = np.zeros(num_epochs)\n",
        "seq=10\n",
        "# Number of steps to unroll\n",
        "seq_dim =seq-1  \n",
        "\n",
        "patience=50\n",
        "check_early_stopping=0\n",
        "val_accuracy=0\n",
        "test_accuracy=0\n",
        "for t in range(num_epochs):\n",
        "    # Initialise hidden state\n",
        "    # Don't do this if you want your LSTM to be stateful\n",
        "    #model.hidden = model.init_hidden()\n",
        "    model.train()\n",
        "    # Forward pass\n",
        "    y_train_pred = model(x_train)\n",
        "    \n",
        "    # print(y_train_pred)\n",
        "    # print(y_train_pred.shape)\n",
        "    # print(y_train)\n",
        "    # print(y_train.shape)\n",
        "    \n",
        "    loss = loss_fn(y_train_pred, y_train)\n",
        "    \n",
        "    \n",
        "    if t % 10 == 0 and t !=0:\n",
        "        print(\"Epoch \", t, \"CE: \", loss.item())\n",
        "    hist[t] = loss.item()\n",
        "    print(\"Epoch: \",t)\n",
        "    print(\"training acc: {}, training loss: {}\".format(get_accuracy(y_train,y_train_pred),loss))\n",
        "    \n",
        "    # Zero out gradient, else they will accumulate between epochs\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimiser.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct=0\n",
        "    with torch.no_grad():\n",
        "      y_val_pred=model(x_val)\n",
        "      val_acc=get_accuracy(y_val,y_val_pred)\n",
        "\n",
        "      y_test_pred=model(x_test)\n",
        "      test_acc=get_accuracy(y_test,y_test_pred)\n",
        "      #print(y_val_pred)\n",
        "      #print(get_accuracy(y_val,y_val_pred))\n",
        "      if val_accuracy <val_acc:\n",
        "        val_accuracy=val_acc\n",
        "        test_accuracy=test_acc\n",
        "        check_early_stopping=0\n",
        "      else:\n",
        "        check_early_stopping=check_early_stopping+1\n",
        "    print(\"val acc: {}, test acc: {}\".format(val_acc,test_acc))\n",
        "    if check_early_stopping==patience:\n",
        "      print(\"early stopping!\")\n",
        "      print(val_accuracy)\n",
        "      print(\"epochs: \",t)\n",
        "      break\n",
        "#test score\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     y_test_pred=model(x_test)\n",
        "#     print(get_accuracy(y_test,y_test_pred))\n",
        "print(\"Best test acc: \",test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPK5Bkh8D3VU",
        "outputId": "f0669d26-821e-48f7-c3ab-475fdb9a1705"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "training acc: 0.517617499261011, training loss: 0.6927374601364136\n",
            "val acc: 0.4857142857142857, test acc: 0.5110294117647058\n",
            "Epoch:  1\n",
            "training acc: 0.5194501921371564, training loss: 0.692336916923523\n",
            "val acc: 0.48599439775910364, test acc: 0.5154411764705882\n",
            "Epoch:  2\n",
            "training acc: 0.5180313331362696, training loss: 0.6921706199645996\n",
            "val acc: 0.48851540616246497, test acc: 0.5119485294117647\n",
            "Epoch:  3\n",
            "training acc: 0.5193910730121194, training loss: 0.6920056343078613\n",
            "val acc: 0.48403361344537815, test acc: 0.5141544117647059\n",
            "Epoch:  4\n",
            "training acc: 0.5205438959503399, training loss: 0.6918320655822754\n",
            "val acc: 0.47675070028011207, test acc: 0.5167279411764706\n",
            "Epoch:  5\n",
            "training acc: 0.5229086609518179, training loss: 0.6916871070861816\n",
            "val acc: 0.4745098039215686, test acc: 0.5165441176470589\n",
            "Epoch:  6\n",
            "training acc: 0.522849541826781, training loss: 0.691580593585968\n",
            "val acc: 0.47198879551820727, test acc: 0.5165441176470589\n",
            "Epoch:  7\n",
            "training acc: 0.5228199822642625, training loss: 0.6914927363395691\n",
            "val acc: 0.47058823529411764, test acc: 0.5167279411764706\n",
            "Epoch:  8\n",
            "training acc: 0.5236772095772982, training loss: 0.6913993954658508\n",
            "val acc: 0.46554621848739497, test acc: 0.5159926470588235\n",
            "Epoch:  9\n",
            "training acc: 0.5241501625775938, training loss: 0.6912940740585327\n",
            "val acc: 0.47086834733893557, test acc: 0.5159926470588235\n",
            "Epoch  10 CE:  0.691184401512146\n",
            "Epoch:  10\n",
            "training acc: 0.5253325450783328, training loss: 0.691184401512146\n",
            "val acc: 0.4703081232492997, test acc: 0.5193014705882353\n",
            "Epoch:  11\n",
            "training acc: 0.5264558084540348, training loss: 0.6910802721977234\n",
            "val acc: 0.47703081232493, test acc: 0.5169117647058824\n",
            "Epoch:  12\n",
            "training acc: 0.526899201891812, training loss: 0.690984308719635\n",
            "val acc: 0.4756302521008403, test acc: 0.5165441176470589\n",
            "Epoch:  13\n",
            "training acc: 0.5281111439550694, training loss: 0.690891683101654\n",
            "val acc: 0.4745098039215686, test acc: 0.5154411764705882\n",
            "Epoch:  14\n",
            "training acc: 0.5290274903931422, training loss: 0.6907975077629089\n",
            "val acc: 0.47478991596638653, test acc: 0.5163602941176471\n",
            "Epoch:  15\n",
            "training acc: 0.5289683712681053, training loss: 0.6907032132148743\n",
            "val acc: 0.4742296918767507, test acc: 0.5185661764705882\n",
            "Epoch:  16\n",
            "training acc: 0.5294413242684008, training loss: 0.6906136870384216\n",
            "val acc: 0.473109243697479, test acc: 0.5193014705882353\n",
            "Epoch:  17\n",
            "training acc: 0.5296778007685486, training loss: 0.6905292868614197\n",
            "val acc: 0.4725490196078431, test acc: 0.5205882352941177\n",
            "Epoch:  18\n",
            "training acc: 0.5302394324563996, training loss: 0.6904429197311401\n",
            "val acc: 0.47058823529411764, test acc: 0.5207720588235294\n",
            "Epoch:  19\n",
            "training acc: 0.5312148980195093, training loss: 0.6903473138809204\n",
            "val acc: 0.473109243697479, test acc: 0.5194852941176471\n",
            "Epoch  20 CE:  0.690243661403656\n",
            "Epoch:  20\n",
            "training acc: 0.5319834466449896, training loss: 0.690243661403656\n",
            "val acc: 0.4714285714285714, test acc: 0.5161764705882353\n",
            "Epoch:  21\n",
            "training acc: 0.5323381613952114, training loss: 0.6901392340660095\n",
            "val acc: 0.47366946778711483, test acc: 0.5159926470588235\n",
            "Epoch:  22\n",
            "training acc: 0.533875258646172, training loss: 0.6900371313095093\n",
            "val acc: 0.4722689075630252, test acc: 0.5159926470588235\n",
            "Epoch:  23\n",
            "training acc: 0.5336979012710612, training loss: 0.689932644367218\n",
            "val acc: 0.4722689075630252, test acc: 0.515625\n",
            "Epoch:  24\n",
            "training acc: 0.5339639373337275, training loss: 0.6898213624954224\n",
            "val acc: 0.4725490196078431, test acc: 0.5159926470588235\n",
            "Epoch:  25\n",
            "training acc: 0.5344664498965416, training loss: 0.6897061467170715\n",
            "val acc: 0.4725490196078431, test acc: 0.5170955882352941\n",
            "Epoch:  26\n",
            "training acc: 0.5354419154596513, training loss: 0.6895889639854431\n",
            "val acc: 0.4703081232492997, test acc: 0.5176470588235295\n",
            "Epoch:  27\n",
            "training acc: 0.5350872007094295, training loss: 0.6894632577896118\n",
            "val acc: 0.46834733893557423, test acc: 0.5159926470588235\n",
            "Epoch:  28\n",
            "training acc: 0.5355601537097251, training loss: 0.6893253922462463\n",
            "val acc: 0.4689075630252101, test acc: 0.5147058823529411\n",
            "Epoch:  29\n",
            "training acc: 0.535678391959799, training loss: 0.6891811490058899\n",
            "val acc: 0.4714285714285714, test acc: 0.5148897058823529\n",
            "Epoch  30 CE:  0.6890314221382141\n",
            "Epoch:  30\n",
            "training acc: 0.5366242979603902, training loss: 0.6890314221382141\n",
            "val acc: 0.4703081232492997, test acc: 0.5172794117647059\n",
            "Epoch:  31\n",
            "training acc: 0.5371268105232042, training loss: 0.6888729333877563\n",
            "val acc: 0.46974789915966386, test acc: 0.51875\n",
            "Epoch:  32\n",
            "training acc: 0.5376293230860183, training loss: 0.6887104511260986\n",
            "val acc: 0.4725490196078431, test acc: 0.5194852941176471\n",
            "Epoch:  33\n",
            "training acc: 0.5387525864617204, training loss: 0.6885398626327515\n",
            "val acc: 0.4764705882352941, test acc: 0.5207720588235294\n",
            "Epoch:  34\n",
            "training acc: 0.5394028968371268, training loss: 0.6883583664894104\n",
            "val acc: 0.47871148459383756, test acc: 0.5200367647058823\n",
            "Epoch:  35\n",
            "training acc: 0.5393142181495714, training loss: 0.6881743669509888\n",
            "val acc: 0.4789915966386555, test acc: 0.5193014705882353\n",
            "Epoch:  36\n",
            "training acc: 0.5405261602128288, training loss: 0.6879847049713135\n",
            "val acc: 0.4801120448179272, test acc: 0.5207720588235294\n",
            "Epoch:  37\n",
            "training acc: 0.5420336979012711, training loss: 0.6877947449684143\n",
            "val acc: 0.484593837535014, test acc: 0.5183823529411765\n",
            "Epoch:  38\n",
            "training acc: 0.5422701744014189, training loss: 0.687601625919342\n",
            "val acc: 0.4857142857142857, test acc: 0.5152573529411765\n",
            "Epoch:  39\n",
            "training acc: 0.5422110552763819, training loss: 0.6874083280563354\n",
            "val acc: 0.4879551820728291, test acc: 0.5137867647058824\n",
            "Epoch  40 CE:  0.6872155666351318\n",
            "Epoch:  40\n",
            "training acc: 0.5434821164646764, training loss: 0.6872155666351318\n",
            "val acc: 0.48739495798319327, test acc: 0.5121323529411764\n",
            "Epoch:  41\n",
            "training acc: 0.5448122967780077, training loss: 0.687028169631958\n",
            "val acc: 0.4907563025210084, test acc: 0.5117647058823529\n",
            "Epoch:  42\n",
            "training acc: 0.5455217262784511, training loss: 0.6868743300437927\n",
            "val acc: 0.4946778711484594, test acc: 0.5130514705882353\n",
            "Epoch:  43\n",
            "training acc: 0.5443984629027491, training loss: 0.6867555975914001\n",
            "val acc: 0.49607843137254903, test acc: 0.5112132352941177\n",
            "Epoch:  44\n",
            "training acc: 0.5458173219036359, training loss: 0.6865187287330627\n",
            "val acc: 0.4946778711484594, test acc: 0.5071691176470589\n",
            "Epoch:  45\n",
            "training acc: 0.5458764410286727, training loss: 0.68634432554245\n",
            "val acc: 0.49131652661064423, test acc: 0.5079044117647059\n",
            "Epoch:  46\n",
            "training acc: 0.5467336683417086, training loss: 0.686224639415741\n",
            "val acc: 0.4935574229691877, test acc: 0.506985294117647\n",
            "Epoch:  47\n",
            "training acc: 0.5466745492166716, training loss: 0.6859761476516724\n",
            "val acc: 0.4946778711484594, test acc: 0.5047794117647059\n",
            "Epoch:  48\n",
            "training acc: 0.5464676322790423, training loss: 0.6858364343643188\n",
            "val acc: 0.49411764705882355, test acc: 0.50625\n",
            "Epoch:  49\n",
            "training acc: 0.5482412060301508, training loss: 0.685648500919342\n",
            "val acc: 0.4943977591036415, test acc: 0.50625\n",
            "Epoch  50 CE:  0.6854198575019836\n",
            "Epoch:  50\n",
            "training acc: 0.547324859592078, training loss: 0.6854198575019836\n",
            "val acc: 0.49159663865546216, test acc: 0.5033088235294118\n",
            "Epoch:  51\n",
            "training acc: 0.5483003251551877, training loss: 0.6852773427963257\n",
            "val acc: 0.4946778711484594, test acc: 0.50625\n",
            "Epoch:  52\n",
            "training acc: 0.5488028377180018, training loss: 0.6850389242172241\n",
            "val acc: 0.49495798319327733, test acc: 0.5051470588235294\n",
            "Epoch:  53\n",
            "training acc: 0.5488028377180018, training loss: 0.6848406791687012\n",
            "val acc: 0.4910364145658263, test acc: 0.5047794117647059\n",
            "Epoch:  54\n",
            "training acc: 0.5504286136565179, training loss: 0.6846542954444885\n",
            "val acc: 0.49411764705882355, test acc: 0.5064338235294118\n",
            "Epoch:  55\n",
            "training acc: 0.5498965415311854, training loss: 0.6843921542167664\n",
            "val acc: 0.49131652661064423, test acc: 0.5053308823529412\n",
            "Epoch:  56\n",
            "training acc: 0.5499556606562223, training loss: 0.6841862201690674\n",
            "val acc: 0.4907563025210084, test acc: 0.5047794117647059\n",
            "Epoch:  57\n",
            "training acc: 0.5519065917824416, training loss: 0.6839536428451538\n",
            "val acc: 0.49019607843137253, test acc: 0.5060661764705883\n",
            "Epoch:  58\n",
            "training acc: 0.552616021282885, training loss: 0.6836670637130737\n",
            "val acc: 0.48851540616246497, test acc: 0.5055147058823529\n",
            "Epoch:  59\n",
            "training acc: 0.5536210464085132, training loss: 0.6834160089492798\n",
            "val acc: 0.492436974789916, test acc: 0.5011029411764706\n",
            "Epoch  60 CE:  0.6831679344177246\n",
            "Epoch:  60\n",
            "training acc: 0.5540348802837718, training loss: 0.6831679344177246\n",
            "val acc: 0.4887955182072829, test acc: 0.5034926470588236\n",
            "Epoch:  61\n",
            "training acc: 0.5566361217853976, training loss: 0.6828751564025879\n",
            "val acc: 0.4899159663865546, test acc: 0.502389705882353\n",
            "Epoch:  62\n",
            "training acc: 0.5569908365356193, training loss: 0.6825577020645142\n",
            "val acc: 0.49019607843137253, test acc: 0.5009191176470589\n",
            "Epoch:  63\n",
            "training acc: 0.5573751108483594, training loss: 0.6822527647018433\n",
            "val acc: 0.48935574229691875, test acc: 0.4985294117647059\n",
            "Epoch:  64\n",
            "training acc: 0.5587348507242093, training loss: 0.6819679737091064\n",
            "val acc: 0.48487394957983193, test acc: 0.4963235294117647\n",
            "Epoch:  65\n",
            "training acc: 0.5582323381613952, training loss: 0.6817169785499573\n",
            "val acc: 0.4857142857142857, test acc: 0.49926470588235294\n",
            "Epoch:  66\n",
            "training acc: 0.5602423884126515, training loss: 0.6815176606178284\n",
            "val acc: 0.48823529411764705, test acc: 0.49319852941176473\n",
            "Epoch:  67\n",
            "training acc: 0.5598285545373929, training loss: 0.6812163591384888\n",
            "val acc: 0.48823529411764705, test acc: 0.4974264705882353\n",
            "Epoch:  68\n",
            "training acc: 0.5622524386639077, training loss: 0.6807646155357361\n",
            "val acc: 0.4826330532212885, test acc: 0.49356617647058826\n",
            "Epoch:  69\n",
            "training acc: 0.564321608040201, training loss: 0.6804962158203125\n",
            "val acc: 0.48823529411764705, test acc: 0.4948529411764706\n",
            "Epoch  70 CE:  0.6803293824195862\n",
            "Epoch:  70\n",
            "training acc: 0.5618090452261306, training loss: 0.6803293824195862\n",
            "val acc: 0.4854341736694678, test acc: 0.49356617647058826\n",
            "Epoch:  71\n",
            "training acc: 0.5643511676027195, training loss: 0.6799431443214417\n",
            "val acc: 0.48515406162464986, test acc: 0.4920955882352941\n",
            "Epoch:  72\n",
            "training acc: 0.5661543009163464, training loss: 0.6795671582221985\n",
            "val acc: 0.48851540616246497, test acc: 0.49356617647058826\n",
            "Epoch:  73\n",
            "training acc: 0.5643807271652379, training loss: 0.6793980002403259\n",
            "val acc: 0.4876750700280112, test acc: 0.49301470588235297\n",
            "Epoch:  74\n",
            "training acc: 0.5673071238545669, training loss: 0.6791067123413086\n",
            "val acc: 0.484593837535014, test acc: 0.490625\n",
            "Epoch:  75\n",
            "training acc: 0.5668341708542713, training loss: 0.6786813139915466\n",
            "val acc: 0.48711484593837534, test acc: 0.4924632352941177\n",
            "Epoch:  76\n",
            "training acc: 0.5664203369790127, training loss: 0.6784290075302124\n",
            "val acc: 0.4887955182072829, test acc: 0.4943014705882353\n",
            "Epoch:  77\n",
            "training acc: 0.5682825894176766, training loss: 0.678191065788269\n",
            "val acc: 0.49131652661064423, test acc: 0.4884191176470588\n",
            "Epoch:  78\n",
            "training acc: 0.5684599467927874, training loss: 0.6778059601783752\n",
            "val acc: 0.4918767507002801, test acc: 0.49136029411764703\n",
            "Epoch:  79\n",
            "training acc: 0.5671888856044931, training loss: 0.677513599395752\n",
            "val acc: 0.4876750700280112, test acc: 0.48768382352941175\n",
            "Epoch  80 CE:  0.6772393584251404\n",
            "Epoch:  80\n",
            "training acc: 0.5703517587939698, training loss: 0.6772393584251404\n",
            "val acc: 0.4955182072829132, test acc: 0.48933823529411763\n",
            "Epoch:  81\n",
            "training acc: 0.570233520543896, training loss: 0.676797091960907\n",
            "val acc: 0.4943977591036415, test acc: 0.4915441176470588\n",
            "Epoch:  82\n",
            "training acc: 0.5703221992314513, training loss: 0.6764876246452332\n",
            "val acc: 0.4890756302521008, test acc: 0.48511029411764706\n",
            "Epoch:  83\n",
            "training acc: 0.5720662134200414, training loss: 0.6762353777885437\n",
            "val acc: 0.4946778711484594, test acc: 0.48933823529411763\n",
            "Epoch:  84\n",
            "training acc: 0.5724504877327815, training loss: 0.6758323311805725\n",
            "val acc: 0.5002801120448179, test acc: 0.4875\n",
            "Epoch:  85\n",
            "training acc: 0.5729825598581141, training loss: 0.6755799651145935\n",
            "val acc: 0.4887955182072829, test acc: 0.48713235294117646\n",
            "Epoch:  86\n",
            "training acc: 0.5738989062961868, training loss: 0.6756945252418518\n",
            "val acc: 0.4946778711484594, test acc: 0.48253676470588236\n",
            "Epoch:  87\n",
            "training acc: 0.5730121194206326, training loss: 0.6759904623031616\n",
            "val acc: 0.49719887955182074, test acc: 0.4970588235294118\n",
            "Epoch:  88\n",
            "training acc: 0.5744900975465563, training loss: 0.6752134561538696\n",
            "val acc: 0.4899159663865546, test acc: 0.48713235294117646\n",
            "Epoch:  89\n",
            "training acc: 0.5762045521726279, training loss: 0.6744247078895569\n",
            "val acc: 0.4980392156862745, test acc: 0.484375\n",
            "Epoch  90 CE:  0.6747609972953796\n",
            "Epoch:  90\n",
            "training acc: 0.575465563109666, training loss: 0.6747609972953796\n",
            "val acc: 0.4997198879551821, test acc: 0.49301470588235297\n",
            "Epoch:  91\n",
            "training acc: 0.5755838013597399, training loss: 0.673846960067749\n",
            "val acc: 0.4935574229691877, test acc: 0.4869485294117647\n",
            "Epoch:  92\n",
            "training acc: 0.5762341117351463, training loss: 0.6739346981048584\n",
            "val acc: 0.49635854341736696, test acc: 0.4834558823529412\n",
            "Epoch:  93\n",
            "training acc: 0.5783624002364764, training loss: 0.673376739025116\n",
            "val acc: 0.5044817927170868, test acc: 0.48216911764705883\n",
            "Epoch:  94\n",
            "training acc: 0.5787171149866982, training loss: 0.6731513738632202\n",
            "val acc: 0.49943977591036415, test acc: 0.4886029411764706\n",
            "Epoch:  95\n",
            "training acc: 0.5770617794856636, training loss: 0.6728799939155579\n",
            "val acc: 0.49607843137254903, test acc: 0.4873161764705882\n",
            "Epoch:  96\n",
            "training acc: 0.578835353236772, training loss: 0.6723945736885071\n",
            "val acc: 0.49943977591036415, test acc: 0.4818014705882353\n",
            "Epoch:  97\n",
            "training acc: 0.5811409991132132, training loss: 0.672358512878418\n",
            "val acc: 0.5028011204481793, test acc: 0.4880514705882353\n",
            "Epoch:  98\n",
            "training acc: 0.5803428909252143, training loss: 0.6716815233230591\n",
            "val acc: 0.4969187675070028, test acc: 0.4904411764705882\n",
            "Epoch:  99\n",
            "training acc: 0.5802246526751405, training loss: 0.6717135310173035\n",
            "val acc: 0.5014005602240896, test acc: 0.484375\n",
            "Epoch  100 CE:  0.6711200475692749\n",
            "Epoch:  100\n",
            "training acc: 0.5824120603015075, training loss: 0.6711200475692749\n",
            "val acc: 0.5056022408963585, test acc: 0.4829044117647059\n",
            "Epoch:  101\n",
            "training acc: 0.583683121489802, training loss: 0.6708788275718689\n",
            "val acc: 0.49831932773109244, test acc: 0.49080882352941174\n",
            "Epoch:  102\n",
            "training acc: 0.5834466449896542, training loss: 0.6706768870353699\n",
            "val acc: 0.5016806722689076, test acc: 0.4869485294117647\n",
            "Epoch:  103\n",
            "training acc: 0.5834762045521726, training loss: 0.6700617671012878\n",
            "val acc: 0.5047619047619047, test acc: 0.4852941176470588\n",
            "Epoch:  104\n",
            "training acc: 0.5847177061779486, training loss: 0.6699791550636292\n",
            "val acc: 0.4988795518207283, test acc: 0.4900735294117647\n",
            "Epoch:  105\n",
            "training acc: 0.585368016553355, training loss: 0.6695863008499146\n",
            "val acc: 0.5011204481792717, test acc: 0.4880514705882353\n",
            "Epoch:  106\n",
            "training acc: 0.5848359444280224, training loss: 0.6690472960472107\n",
            "val acc: 0.507563025210084, test acc: 0.48915441176470587\n",
            "Epoch:  107\n",
            "training acc: 0.586491279929057, training loss: 0.6689248085021973\n",
            "val acc: 0.4977591036414566, test acc: 0.48933823529411763\n",
            "Epoch:  108\n",
            "training acc: 0.5866390777416495, training loss: 0.6685519814491272\n",
            "val acc: 0.5061624649859944, test acc: 0.48933823529411763\n",
            "Epoch:  109\n",
            "training acc: 0.5870824711794266, training loss: 0.6679861545562744\n",
            "val acc: 0.5061624649859944, test acc: 0.4920955882352941\n",
            "Epoch  110 CE:  0.6677364110946655\n",
            "Epoch:  110\n",
            "training acc: 0.5884126514927579, training loss: 0.6677364110946655\n",
            "val acc: 0.49747899159663866, test acc: 0.4898897058823529\n",
            "Epoch:  111\n",
            "training acc: 0.5892994383683121, training loss: 0.6675155162811279\n",
            "val acc: 0.503641456582633, test acc: 0.490625\n",
            "Epoch:  112\n",
            "training acc: 0.5902749039314218, training loss: 0.6670161485671997\n",
            "val acc: 0.5058823529411764, test acc: 0.4915441176470588\n",
            "Epoch:  113\n",
            "training acc: 0.5906296186816435, training loss: 0.6664955019950867\n",
            "val acc: 0.5016806722689076, test acc: 0.4904411764705882\n",
            "Epoch:  114\n",
            "training acc: 0.5909252143068282, training loss: 0.6661681532859802\n",
            "val acc: 0.5056022408963585, test acc: 0.4904411764705882\n",
            "Epoch:  115\n",
            "training acc: 0.5922553946201596, training loss: 0.6659458875656128\n",
            "val acc: 0.4946778711484594, test acc: 0.48933823529411763\n",
            "Epoch:  116\n",
            "training acc: 0.5913094886195684, training loss: 0.6656779050827026\n",
            "val acc: 0.503921568627451, test acc: 0.4895220588235294\n",
            "Epoch:  117\n",
            "training acc: 0.5936446940585279, training loss: 0.6652786135673523\n",
            "val acc: 0.49943977591036415, test acc: 0.4922794117647059\n",
            "Epoch:  118\n",
            "training acc: 0.5937924918711203, training loss: 0.6647774577140808\n",
            "val acc: 0.49747899159663866, test acc: 0.48970588235294116\n",
            "Epoch:  119\n",
            "training acc: 0.5953000295595625, training loss: 0.6643011569976807\n",
            "val acc: 0.4977591036414566, test acc: 0.48933823529411763\n",
            "Epoch  120 CE:  0.6640450954437256\n",
            "Epoch:  120\n",
            "training acc: 0.5949748743718593, training loss: 0.6640450954437256\n",
            "val acc: 0.4966386554621849, test acc: 0.4920955882352941\n",
            "Epoch:  121\n",
            "training acc: 0.5949748743718593, training loss: 0.6641377210617065\n",
            "val acc: 0.4997198879551821, test acc: 0.48768382352941175\n",
            "Epoch:  122\n",
            "training acc: 0.5944723618090453, training loss: 0.6640838980674744\n",
            "val acc: 0.4918767507002801, test acc: 0.49375\n",
            "Epoch:  123\n",
            "training acc: 0.5976647945610405, training loss: 0.6629738211631775\n",
            "val acc: 0.49047619047619045, test acc: 0.4928308823529412\n",
            "Epoch:  124\n",
            "training acc: 0.5969258054980786, training loss: 0.6627563238143921\n",
            "val acc: 0.5005602240896359, test acc: 0.4873161764705882\n",
            "Epoch:  125\n",
            "training acc: 0.5967780076854863, training loss: 0.6628187298774719\n",
            "val acc: 0.4955182072829132, test acc: 0.4919117647058823\n",
            "Epoch:  126\n",
            "training acc: 0.5983151049364469, training loss: 0.6617617011070251\n",
            "val acc: 0.4935574229691877, test acc: 0.4904411764705882\n",
            "Epoch:  127\n",
            "training acc: 0.6, training loss: 0.6617668271064758\n",
            "val acc: 0.49943977591036415, test acc: 0.48713235294117646\n",
            "Epoch:  128\n",
            "training acc: 0.5989654153118534, training loss: 0.6614054441452026\n",
            "val acc: 0.4966386554621849, test acc: 0.49117647058823527\n",
            "Epoch:  129\n",
            "training acc: 0.6002364765001478, training loss: 0.6606451272964478\n",
            "val acc: 0.49159663865546216, test acc: 0.49264705882352944\n",
            "Epoch  130 CE:  0.660781741142273\n",
            "Epoch:  130\n",
            "training acc: 0.6009163464380727, training loss: 0.660781741142273\n",
            "val acc: 0.49607843137254903, test acc: 0.4900735294117647\n",
            "Epoch:  131\n",
            "training acc: 0.6014779781259237, training loss: 0.6600710153579712\n",
            "val acc: 0.4980392156862745, test acc: 0.4884191176470588\n",
            "Epoch:  132\n",
            "training acc: 0.6013892994383683, training loss: 0.6598202586174011\n",
            "val acc: 0.4910364145658263, test acc: 0.4924632352941177\n",
            "Epoch:  133\n",
            "training acc: 0.6007685486254803, training loss: 0.6598434448242188\n",
            "val acc: 0.49635854341736696, test acc: 0.4909926470588235\n",
            "Epoch:  134\n",
            "training acc: 0.6010050251256281, training loss: 0.659635603427887\n",
            "val acc: 0.4997198879551821, test acc: 0.48621323529411764\n",
            "Epoch:  135\n",
            "training acc: 0.5986402601241502, training loss: 0.6605907678604126\n",
            "val acc: 0.4896358543417367, test acc: 0.49558823529411766\n",
            "Epoch:  136\n",
            "training acc: 0.600059119125037, training loss: 0.6605945229530334\n",
            "val acc: 0.49299719887955185, test acc: 0.4919117647058823\n",
            "Epoch:  137\n",
            "training acc: 0.6035175879396985, training loss: 0.6586302518844604\n",
            "val acc: 0.49859943977591037, test acc: 0.4858455882352941\n",
            "Epoch:  138\n",
            "training acc: 0.604167898315105, training loss: 0.6580227613449097\n",
            "val acc: 0.4955182072829132, test acc: 0.4898897058823529\n",
            "Epoch:  139\n",
            "training acc: 0.6010050251256281, training loss: 0.6585953831672668\n",
            "val acc: 0.4918767507002801, test acc: 0.49136029411764703\n",
            "Epoch  140 CE:  0.6576651930809021\n",
            "Epoch:  140\n",
            "training acc: 0.6042565770026603, training loss: 0.6576651930809021\n",
            "val acc: 0.49523809523809526, test acc: 0.48768382352941175\n",
            "Epoch:  141\n",
            "training acc: 0.6051138043156962, training loss: 0.6569178700447083\n",
            "val acc: 0.4966386554621849, test acc: 0.4878676470588235\n",
            "Epoch:  142\n",
            "training acc: 0.6050842447531777, training loss: 0.6574808359146118\n",
            "val acc: 0.4938375350140056, test acc: 0.49117647058823527\n",
            "Epoch:  143\n",
            "training acc: 0.6053798403783625, training loss: 0.6563028693199158\n",
            "val acc: 0.4910364145658263, test acc: 0.490625\n",
            "Epoch:  144\n",
            "training acc: 0.6059414720662134, training loss: 0.6561598181724548\n",
            "val acc: 0.4980392156862745, test acc: 0.48933823529411763\n",
            "Epoch:  145\n",
            "training acc: 0.6066213420041383, training loss: 0.6562682390213013\n",
            "val acc: 0.492436974789916, test acc: 0.4880514705882353\n",
            "Epoch:  146\n",
            "training acc: 0.6078924031924328, training loss: 0.6552029252052307\n",
            "val acc: 0.492156862745098, test acc: 0.4895220588235294\n",
            "Epoch:  147\n",
            "training acc: 0.6072125332545079, training loss: 0.6553188562393188\n",
            "val acc: 0.49523809523809526, test acc: 0.49080882352941174\n",
            "Epoch:  148\n",
            "training acc: 0.6081288796925806, training loss: 0.6550438404083252\n",
            "val acc: 0.4935574229691877, test acc: 0.4909926470588235\n",
            "Epoch:  149\n",
            "training acc: 0.6093703813183565, training loss: 0.654338538646698\n",
            "val acc: 0.4910364145658263, test acc: 0.4875\n",
            "Epoch  150 CE:  0.6541921496391296\n",
            "Epoch:  150\n",
            "training acc: 0.6088974283180609, training loss: 0.6541921496391296\n",
            "val acc: 0.4943977591036415, test acc: 0.4904411764705882\n",
            "Epoch:  151\n",
            "training acc: 0.6098137747561336, training loss: 0.6539419293403625\n",
            "val acc: 0.49327731092436977, test acc: 0.48823529411764705\n",
            "Epoch:  152\n",
            "training acc: 0.6089269878805794, training loss: 0.6535103917121887\n",
            "val acc: 0.4935574229691877, test acc: 0.4875\n",
            "Epoch:  153\n",
            "training acc: 0.6103162873189477, training loss: 0.65299391746521\n",
            "val acc: 0.4935574229691877, test acc: 0.48915441176470587\n",
            "Epoch:  154\n",
            "training acc: 0.6121489801950931, training loss: 0.6527666449546814\n",
            "val acc: 0.4946778711484594, test acc: 0.48878676470588234\n",
            "Epoch:  155\n",
            "training acc: 0.6101980490688738, training loss: 0.6526455879211426\n",
            "val acc: 0.49411764705882355, test acc: 0.4880514705882353\n",
            "Epoch:  156\n",
            "training acc: 0.6131540053207213, training loss: 0.6520266532897949\n",
            "val acc: 0.4935574229691877, test acc: 0.4886029411764706\n",
            "early stopping!\n",
            "0.507563025210084\n",
            "epochs:  156\n",
            "Best test acc:  0.48915441176470587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "#####################\n",
        "input_dim = 11\n",
        "hidden_dim = 64\n",
        "num_layers = 1 \n",
        "output_dim = 1\n",
        "\n",
        "\n",
        "# Here we define our model as a class\n",
        "class ALSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(ALSTM, self).__init__()\n",
        "        self.softmax_layer=nn.Softmax(dim=1)\n",
        "        #feature transformerlayer\n",
        "        self.feature_transformer=nn.Linear(input_dim,hidden_dim)\n",
        "        self.tanh_layer=nn.Tanh()\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x=self.feature_transformer(x)\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x=self.tanh_layer(x)\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        sumscore=0\n",
        "        scorelist=[]\n",
        "        #print(out[:,-1,:].shape)#torch.Size([98750, 32])\n",
        "        #print(out[:,:,:].shape) #torch.Size([98750, 10, 32])\n",
        "        \n",
        "        base=out[:,-1,:].view(-1,1,self.hidden_dim)\n",
        "        #print(base.shape)\n",
        "        #print(base.transpose(1,2))\n",
        "        #print(base.transpose(1,2).shape)\n",
        "        #input()\n",
        "        base=base.transpose(1,2)\n",
        "        score=torch.bmm(out,base)\n",
        "        #print(score.shape) #torch.Size([98750, 10, 1])\n",
        "        #print(score[0])\n",
        "        score = self.softmax_layer(score) #softmax score \n",
        "        #print(score[0])\n",
        "        #print(score.shape) #torch.Size([98750, 10, 1])\n",
        "        transpose_out=out[:,:,:].transpose(1,2) #torch.Size([98750, 32, 10])\n",
        "        new_out=torch.bmm(transpose_out,score)\n",
        "        #print(new_out.shape)\n",
        "        new_out=new_out.view(-1,self.hidden_dim)\n",
        "        #print(new_out.shape)\n",
        "        #input()\n",
        "        # for i in range(seq):\n",
        "        #   print(out[:,i,:].shape)\n",
        "        #   print(base.shape)\n",
        "        #   score=torch.bmm(out[:,i,:],base)\n",
        "        #   print(score.shape)\n",
        "        #   scorelist.append(score)\n",
        "        #   sumscore=sumscore+score\n",
        "        # print(scorelist)\n",
        "        # print(sumscore)\n",
        "        # input()\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 32, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states!\n",
        "        #print(out[:, -1, :])\n",
        "        #print(out[:, -1, :].shape) \n",
        "        out = self.fc(new_out)\n",
        "        \n",
        "        #input() \n",
        "        # out.size() --> 100, 10\n",
        "        return out\n",
        "    \n",
        "model = ALSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "print(model)\n",
        "print(len(list(model.parameters())))\n",
        "for i in range(len(list(model.parameters()))):\n",
        "    print(list(model.parameters())[i].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjJLiF8r2FT4",
        "outputId": "4572e73c-99d3-4411-9851-1803c017aae1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALSTM(\n",
            "  (softmax_layer): Softmax(dim=1)\n",
            "  (feature_transformer): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (tanh_layer): Tanh()\n",
            "  (lstm): LSTM(64, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "8\n",
            "torch.Size([64, 11])\n",
            "torch.Size([64])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256, 64])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n",
            "torch.Size([1, 64])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "#####################\n",
        "num_epochs = 200\n",
        "hist = np.zeros(num_epochs)\n",
        "seq=10\n",
        "# Number of steps to unroll\n",
        "seq_dim =seq-1  \n",
        "\n",
        "patience=100\n",
        "check_early_stopping=0\n",
        "val_accuracy=0\n",
        "test_accuracy=0\n",
        "for t in range(num_epochs):\n",
        "    # Initialise hidden state\n",
        "    # Don't do this if you want your LSTM to be stateful\n",
        "    #model.hidden = model.init_hidden()\n",
        "    model.train()\n",
        "    # Forward pass\n",
        "    y_train_pred = model(x_train)\n",
        "    \n",
        "    # print(y_train_pred)\n",
        "    # print(y_train_pred.shape)\n",
        "    # print(y_train)\n",
        "    # print(y_train.shape)\n",
        "    \n",
        "    loss = loss_fn(y_train_pred, y_train)\n",
        "    \n",
        "    \n",
        "    if t % 10 == 0 and t !=0:\n",
        "        print(\"Epoch \", t, \"CE: \", loss.item())\n",
        "    hist[t] = loss.item()\n",
        "    print(\"Epoch: \",t)\n",
        "    print(\"training acc: {}, training loss: {}\".format(get_accuracy(y_train,y_train_pred),loss))\n",
        "    \n",
        "    # Zero out gradient, else they will accumulate between epochs\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimiser.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct=0\n",
        "    with torch.no_grad():\n",
        "      y_val_pred=model(x_val)\n",
        "      val_acc=get_accuracy(y_val,y_val_pred)\n",
        "\n",
        "      y_test_pred=model(x_test)\n",
        "      test_acc=get_accuracy(y_test,y_test_pred)\n",
        "      #print(y_val_pred)\n",
        "      #print(get_accuracy(y_val,y_val_pred))\n",
        "      if val_accuracy <val_acc:\n",
        "        val_accuracy=val_acc\n",
        "        test_accuracy=test_acc\n",
        "        check_early_stopping=0\n",
        "      else:\n",
        "        check_early_stopping=check_early_stopping+1\n",
        "    print(\"val acc: {}, test acc: {}\".format(val_acc,test_acc))\n",
        "    if check_early_stopping==patience:\n",
        "      print(\"early stopping!\")\n",
        "      print(val_accuracy)\n",
        "      print(\"epochs: \",t)\n",
        "      break\n",
        "#test score\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     y_test_pred=model(x_test)\n",
        "#     print(get_accuracy(y_test,y_test_pred))\n",
        "print(\"Best test acc: \",test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hos4zGWDEB6F",
        "outputId": "52ca4201-2bf2-4aff-f56a-23fe704e4539"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "training acc: 0.490659178244162, training loss: 0.6951509714126587\n",
            "val acc: 0.5411764705882353, test acc: 0.48474264705882353\n",
            "Epoch:  1\n",
            "training acc: 0.4908365356192728, training loss: 0.6937495470046997\n",
            "val acc: 0.5221288515406163, test acc: 0.5119485294117647\n",
            "Epoch:  2\n",
            "training acc: 0.5099320130062075, training loss: 0.692876398563385\n",
            "val acc: 0.48403361344537815, test acc: 0.5169117647058824\n",
            "Epoch:  3\n",
            "training acc: 0.5137451965710907, training loss: 0.6924784779548645\n",
            "val acc: 0.46974789915966386, test acc: 0.5090073529411765\n",
            "Epoch:  4\n",
            "training acc: 0.5154596511971623, training loss: 0.6924772262573242\n",
            "val acc: 0.46750700280112045, test acc: 0.5090073529411765\n",
            "Epoch:  5\n",
            "training acc: 0.5156961276973101, training loss: 0.6926445364952087\n",
            "val acc: 0.4672268907563025, test acc: 0.5097426470588236\n",
            "Epoch:  6\n",
            "training acc: 0.5152231746970145, training loss: 0.6927157640457153\n",
            "val acc: 0.46638655462184875, test acc: 0.5053308823529412\n",
            "Epoch:  7\n",
            "training acc: 0.5156961276973101, training loss: 0.6926302909851074\n",
            "val acc: 0.4666666666666667, test acc: 0.5075367647058824\n",
            "Epoch:  8\n",
            "training acc: 0.5168193910730121, training loss: 0.6924582719802856\n",
            "val acc: 0.46750700280112045, test acc: 0.5073529411764706\n",
            "Epoch:  9\n",
            "training acc: 0.5165829145728643, training loss: 0.692280650138855\n",
            "val acc: 0.4714285714285714, test acc: 0.5064338235294118\n",
            "Epoch  10 CE:  0.6921477913856506\n",
            "Epoch:  10\n",
            "training acc: 0.5165533550103458, training loss: 0.6921477913856506\n",
            "val acc: 0.47478991596638653, test acc: 0.5106617647058823\n",
            "Epoch:  11\n",
            "training acc: 0.5184156074490097, training loss: 0.6920767426490784\n",
            "val acc: 0.4789915966386555, test acc: 0.5126838235294118\n",
            "Epoch:  12\n",
            "training acc: 0.5195093112621934, training loss: 0.692059338092804\n",
            "val acc: 0.4792717086834734, test acc: 0.513235294117647\n",
            "Epoch:  13\n",
            "training acc: 0.518120011823825, training loss: 0.6920705437660217\n",
            "val acc: 0.4857142857142857, test acc: 0.5167279411764706\n",
            "Epoch:  14\n",
            "training acc: 0.5170854271356784, training loss: 0.6920835971832275\n",
            "val acc: 0.48403361344537815, test acc: 0.5196691176470588\n",
            "Epoch:  15\n",
            "training acc: 0.518120011823825, training loss: 0.6920812129974365\n",
            "val acc: 0.4834733893557423, test acc: 0.5209558823529412\n",
            "Epoch:  16\n",
            "training acc: 0.5177357375110848, training loss: 0.6920568943023682\n",
            "val acc: 0.4834733893557423, test acc: 0.5202205882352942\n",
            "Epoch:  17\n",
            "training acc: 0.5187112030741945, training loss: 0.6920111775398254\n",
            "val acc: 0.48207282913165267, test acc: 0.5159926470588235\n",
            "Epoch:  18\n",
            "training acc: 0.518740762636713, training loss: 0.6919496059417725\n",
            "val acc: 0.47478991596638653, test acc: 0.5110294117647058\n",
            "Epoch:  19\n",
            "training acc: 0.519982264262489, training loss: 0.6918825507164001\n",
            "val acc: 0.47366946778711483, test acc: 0.5095588235294117\n",
            "Epoch  20 CE:  0.6918214559555054\n",
            "Epoch:  20\n",
            "training acc: 0.521046408513154, training loss: 0.6918214559555054\n",
            "val acc: 0.47394957983193275, test acc: 0.5099264705882353\n",
            "Epoch:  21\n",
            "training acc: 0.5210759680756725, training loss: 0.6917741894721985\n",
            "val acc: 0.4711484593837535, test acc: 0.5130514705882353\n",
            "Epoch:  22\n",
            "training acc: 0.5206030150753769, training loss: 0.6917423605918884\n",
            "val acc: 0.4711484593837535, test acc: 0.5128676470588235\n",
            "Epoch:  23\n",
            "training acc: 0.5213124445758203, training loss: 0.6917222738265991\n",
            "val acc: 0.469187675070028, test acc: 0.5117647058823529\n",
            "Epoch:  24\n",
            "training acc: 0.5211646467632279, training loss: 0.6917067766189575\n",
            "val acc: 0.46974789915966386, test acc: 0.5119485294117647\n",
            "Epoch:  25\n",
            "training acc: 0.5213715637008572, training loss: 0.6916863322257996\n",
            "val acc: 0.4700280112044818, test acc: 0.5119485294117647\n",
            "Epoch:  26\n",
            "training acc: 0.5218445167011528, training loss: 0.6916530132293701\n",
            "val acc: 0.46946778711484594, test acc: 0.5137867647058824\n",
            "Epoch:  27\n",
            "training acc: 0.5224948270765593, training loss: 0.691605806350708\n",
            "val acc: 0.47170868347338935, test acc: 0.5158088235294118\n",
            "Epoch:  28\n",
            "training acc: 0.5232633757020396, training loss: 0.6915501356124878\n",
            "val acc: 0.4764705882352941, test acc: 0.5172794117647059\n",
            "Epoch:  29\n",
            "training acc: 0.5234702926396689, training loss: 0.6914932727813721\n",
            "val acc: 0.47619047619047616, test acc: 0.5183823529411765\n",
            "Epoch  30 CE:  0.6914408802986145\n",
            "Epoch:  30\n",
            "training acc: 0.5236180904522613, training loss: 0.6914408802986145\n",
            "val acc: 0.4803921568627451, test acc: 0.5222426470588235\n",
            "Epoch:  31\n",
            "training acc: 0.5232633757020396, training loss: 0.6913952827453613\n",
            "val acc: 0.4823529411764706, test acc: 0.5251838235294117\n",
            "Epoch:  32\n",
            "training acc: 0.5243570795152231, training loss: 0.6913547515869141\n",
            "val acc: 0.48515406162464986, test acc: 0.5259191176470588\n",
            "Epoch:  33\n",
            "training acc: 0.5245639964528525, training loss: 0.6913142204284668\n",
            "val acc: 0.48851540616246497, test acc: 0.5272058823529412\n",
            "Epoch:  34\n",
            "training acc: 0.5252438663907775, training loss: 0.691268801689148\n",
            "val acc: 0.48823529411764705, test acc: 0.5264705882352941\n",
            "Epoch:  35\n",
            "training acc: 0.5257463789535914, training loss: 0.6912166476249695\n",
            "val acc: 0.48711484593837534, test acc: 0.5253676470588236\n",
            "Epoch:  36\n",
            "training acc: 0.5253916642033698, training loss: 0.6911584138870239\n",
            "val acc: 0.48711484593837534, test acc: 0.5261029411764706\n",
            "Epoch:  37\n",
            "training acc: 0.5259828554537392, training loss: 0.6910971403121948\n",
            "val acc: 0.4865546218487395, test acc: 0.5270220588235294\n",
            "Epoch:  38\n",
            "training acc: 0.526278451078924, training loss: 0.6910363435745239\n",
            "val acc: 0.4834733893557423, test acc: 0.5266544117647058\n",
            "Epoch:  39\n",
            "training acc: 0.5260419745787762, training loss: 0.6909781098365784\n",
            "val acc: 0.48487394957983193, test acc: 0.5257352941176471\n",
            "Epoch  40 CE:  0.6909220814704895\n",
            "Epoch:  40\n",
            "training acc: 0.5265444871415903, training loss: 0.6909220814704895\n",
            "val acc: 0.4823529411764706, test acc: 0.5284926470588235\n",
            "Epoch:  41\n",
            "training acc: 0.5275495122672185, training loss: 0.6908653378486633\n",
            "val acc: 0.4789915966386555, test acc: 0.5262867647058823\n",
            "Epoch:  42\n",
            "training acc: 0.5278746674549216, training loss: 0.6908044219017029\n",
            "val acc: 0.4803921568627451, test acc: 0.5251838235294117\n",
            "Epoch:  43\n",
            "training acc: 0.528584096955365, training loss: 0.690739095211029\n",
            "val acc: 0.4826330532212885, test acc: 0.5273897058823529\n",
            "Epoch:  44\n",
            "training acc: 0.5284658587052912, training loss: 0.6906708478927612\n",
            "val acc: 0.48599439775910364, test acc: 0.5279411764705882\n",
            "Epoch:  45\n",
            "training acc: 0.5287614543304759, training loss: 0.6906039714813232\n",
            "val acc: 0.484593837535014, test acc: 0.5303308823529411\n",
            "Epoch:  46\n",
            "training acc: 0.5302394324563996, training loss: 0.6905398964881897\n",
            "val acc: 0.4843137254901961, test acc: 0.5318014705882353\n",
            "Epoch:  47\n",
            "training acc: 0.5303281111439551, training loss: 0.6904767155647278\n",
            "val acc: 0.4826330532212885, test acc: 0.5338235294117647\n",
            "Epoch:  48\n",
            "training acc: 0.5306237067691398, training loss: 0.690412163734436\n",
            "val acc: 0.4837535014005602, test acc: 0.5340073529411765\n",
            "Epoch:  49\n",
            "training acc: 0.5313922553946202, training loss: 0.6903439164161682\n",
            "val acc: 0.48487394957983193, test acc: 0.5323529411764706\n",
            "Epoch  50 CE:  0.6902710795402527\n",
            "Epoch:  50\n",
            "training acc: 0.5311262193319539, training loss: 0.6902710795402527\n",
            "val acc: 0.48515406162464986, test acc: 0.5325367647058824\n",
            "Epoch:  51\n",
            "training acc: 0.5315991723322495, training loss: 0.6901941895484924\n",
            "val acc: 0.4868347338935574, test acc: 0.5334558823529412\n",
            "Epoch:  52\n",
            "training acc: 0.5314809340821756, training loss: 0.6901144981384277\n",
            "val acc: 0.48851540616246497, test acc: 0.5349264705882353\n",
            "Epoch:  53\n",
            "training acc: 0.5323972805202483, training loss: 0.6900395750999451\n",
            "val acc: 0.4918767507002801, test acc: 0.5327205882352941\n",
            "Epoch:  54\n",
            "training acc: 0.5319243275199527, training loss: 0.6899707317352295\n",
            "val acc: 0.49047619047619045, test acc: 0.5362132352941177\n",
            "Epoch:  55\n",
            "training acc: 0.5342890925214306, training loss: 0.6898864507675171\n",
            "val acc: 0.4935574229691877, test acc: 0.5327205882352941\n",
            "Epoch:  56\n",
            "training acc: 0.5332840673958026, training loss: 0.68977952003479\n",
            "val acc: 0.49299719887955185, test acc: 0.5295955882352941\n",
            "Epoch:  57\n",
            "training acc: 0.5334909843334319, training loss: 0.6897045969963074\n",
            "val acc: 0.49411764705882355, test acc: 0.5327205882352941\n",
            "Epoch:  58\n",
            "training acc: 0.5330771504581732, training loss: 0.6896101236343384\n",
            "val acc: 0.4966386554621849, test acc: 0.5318014705882353\n",
            "Epoch:  59\n",
            "training acc: 0.5345846881466154, training loss: 0.6894915103912354\n",
            "val acc: 0.49607843137254903, test acc: 0.5303308823529411\n",
            "Epoch  60 CE:  0.6894041299819946\n",
            "Epoch:  60\n",
            "training acc: 0.5352941176470588, training loss: 0.6894041299819946\n",
            "val acc: 0.4946778711484594, test acc: 0.53125\n",
            "Epoch:  61\n",
            "training acc: 0.5366242979603902, training loss: 0.6893038153648376\n",
            "val acc: 0.5011204481792717, test acc: 0.5286764705882353\n",
            "Epoch:  62\n",
            "training acc: 0.5361513449600946, training loss: 0.6891686916351318\n",
            "val acc: 0.5025210084033613, test acc: 0.5251838235294117\n",
            "Epoch:  63\n",
            "training acc: 0.5358557493349099, training loss: 0.6890549659729004\n",
            "val acc: 0.4980392156862745, test acc: 0.5259191176470588\n",
            "Epoch:  64\n",
            "training acc: 0.5384569908365356, training loss: 0.6889588236808777\n",
            "val acc: 0.5050420168067227, test acc: 0.5238970588235294\n",
            "Epoch:  65\n",
            "training acc: 0.5370085722731304, training loss: 0.6888404488563538\n",
            "val acc: 0.49719887955182074, test acc: 0.5231617647058824\n",
            "Epoch:  66\n",
            "training acc: 0.5383683121489802, training loss: 0.6887050867080688\n",
            "val acc: 0.5019607843137255, test acc: 0.5207720588235294\n",
            "Epoch:  67\n",
            "training acc: 0.5381318356488324, training loss: 0.6885697841644287\n",
            "val acc: 0.503641456582633, test acc: 0.5205882352941177\n",
            "Epoch:  68\n",
            "training acc: 0.5380135973987585, training loss: 0.688444197177887\n",
            "val acc: 0.5022408963585434, test acc: 0.5198529411764706\n",
            "Epoch:  69\n",
            "training acc: 0.5387525864617204, training loss: 0.6883238554000854\n",
            "val acc: 0.5061624649859944, test acc: 0.5161764705882353\n",
            "Epoch  70 CE:  0.6882395148277283\n",
            "Epoch:  70\n",
            "training acc: 0.5387230268992019, training loss: 0.6882395148277283\n",
            "val acc: 0.5016806722689076, test acc: 0.5233455882352941\n",
            "Epoch:  71\n",
            "training acc: 0.5398167307123855, training loss: 0.6883878111839294\n",
            "val acc: 0.5117647058823529, test acc: 0.5163602941176471\n",
            "Epoch:  72\n",
            "training acc: 0.5377475613360923, training loss: 0.6884541511535645\n",
            "val acc: 0.5019607843137255, test acc: 0.5169117647058824\n",
            "Epoch:  73\n",
            "training acc: 0.5414425066509015, training loss: 0.6878836154937744\n",
            "val acc: 0.5019607843137255, test acc: 0.5205882352941177\n",
            "Epoch:  74\n",
            "training acc: 0.5411173514631984, training loss: 0.6880969405174255\n",
            "val acc: 0.5050420168067227, test acc: 0.5163602941176471\n",
            "Epoch:  75\n",
            "training acc: 0.5417676618386048, training loss: 0.6877478361129761\n",
            "val acc: 0.511484593837535, test acc: 0.5148897058823529\n",
            "Epoch:  76\n",
            "training acc: 0.5419450192137156, training loss: 0.6877613663673401\n",
            "val acc: 0.5044817927170868, test acc: 0.515625\n",
            "Epoch:  77\n",
            "training acc: 0.5430682825894176, training loss: 0.6875873804092407\n",
            "val acc: 0.503921568627451, test acc: 0.5172794117647059\n",
            "Epoch:  78\n",
            "training acc: 0.5445167011528229, training loss: 0.6874768733978271\n",
            "val acc: 0.5128851540616246, test acc: 0.5158088235294118\n",
            "Epoch:  79\n",
            "training acc: 0.5435412355897132, training loss: 0.6873942613601685\n",
            "val acc: 0.5142857142857142, test acc: 0.51875\n",
            "Epoch  80 CE:  0.6872169971466064\n",
            "Epoch:  80\n",
            "training acc: 0.5445462607153414, training loss: 0.6872169971466064\n",
            "val acc: 0.5050420168067227, test acc: 0.5119485294117647\n",
            "Epoch:  81\n",
            "training acc: 0.5441324268400828, training loss: 0.6871857047080994\n",
            "val acc: 0.5081232492997199, test acc: 0.5117647058823529\n",
            "Epoch:  82\n",
            "training acc: 0.5453443689033403, training loss: 0.686967134475708\n",
            "val acc: 0.5170868347338936, test acc: 0.5143382352941176\n",
            "Epoch:  83\n",
            "training acc: 0.5453739284658587, training loss: 0.6869681477546692\n",
            "val acc: 0.5106442577030812, test acc: 0.5141544117647059\n",
            "Epoch:  84\n",
            "training acc: 0.5464380727165238, training loss: 0.6867130398750305\n",
            "val acc: 0.5072829131652661, test acc: 0.5130514705882353\n",
            "Epoch:  85\n",
            "training acc: 0.5459651197162282, training loss: 0.6867263913154602\n",
            "val acc: 0.5117647058823529, test acc: 0.5108455882352941\n",
            "Epoch:  86\n",
            "training acc: 0.546201596216376, training loss: 0.6864847540855408\n",
            "val acc: 0.515686274509804, test acc: 0.5104779411764706\n",
            "Epoch:  87\n",
            "training acc: 0.5459355601537097, training loss: 0.686432421207428\n",
            "val acc: 0.5095238095238095, test acc: 0.5106617647058823\n",
            "Epoch:  88\n",
            "training acc: 0.5481229677800769, training loss: 0.6862974166870117\n",
            "val acc: 0.5131652661064425, test acc: 0.5099264705882353\n",
            "Epoch:  89\n",
            "training acc: 0.5483003251551877, training loss: 0.6861026287078857\n",
            "val acc: 0.5154061624649859, test acc: 0.5079044117647059\n",
            "Epoch  90 CE:  0.6860787868499756\n",
            "Epoch:  90\n",
            "training acc: 0.5484185634052616, training loss: 0.6860787868499756\n",
            "val acc: 0.5126050420168067, test acc: 0.5073529411764706\n",
            "Epoch:  91\n",
            "training acc: 0.5495122672184451, training loss: 0.6858754754066467\n",
            "val acc: 0.5123249299719888, test acc: 0.508639705882353\n",
            "Epoch:  92\n",
            "training acc: 0.5495418267809636, training loss: 0.6857157349586487\n",
            "val acc: 0.5165266106442578, test acc: 0.5064338235294118\n",
            "Epoch:  93\n",
            "training acc: 0.549187112030742, training loss: 0.6856534481048584\n",
            "val acc: 0.5109243697478991, test acc: 0.5058823529411764\n",
            "Epoch:  94\n",
            "training acc: 0.5492166715932604, training loss: 0.6855142712593079\n",
            "val acc: 0.5100840336134453, test acc: 0.5077205882352941\n",
            "Epoch:  95\n",
            "training acc: 0.5504877327815548, training loss: 0.6853104829788208\n",
            "val acc: 0.5120448179271708, test acc: 0.5066176470588235\n",
            "Epoch:  96\n",
            "training acc: 0.5511676027194797, training loss: 0.6851339340209961\n",
            "val acc: 0.5092436974789916, test acc: 0.5077205882352941\n",
            "Epoch:  97\n",
            "training acc: 0.5505172923440733, training loss: 0.685038149356842\n",
            "val acc: 0.5137254901960784, test acc: 0.508639705882353\n",
            "Epoch:  98\n",
            "training acc: 0.5531776529707361, training loss: 0.6850156784057617\n",
            "val acc: 0.5064425770308123, test acc: 0.5066176470588235\n",
            "Epoch:  99\n",
            "training acc: 0.5524091043452557, training loss: 0.685069739818573\n",
            "val acc: 0.5137254901960784, test acc: 0.5088235294117647\n",
            "Epoch  100 CE:  0.6850108504295349\n",
            "Epoch:  100\n",
            "training acc: 0.551492757907183, training loss: 0.6850108504295349\n",
            "val acc: 0.5070028011204482, test acc: 0.5068014705882353\n",
            "early stopping!\n",
            "0.5411764705882353\n",
            "epochs:  100\n",
            "Best test acc:  0.48474264705882353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout,device \n",
        "                                                  ) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        \n",
        "        \n",
        "        for layer in self.layers:\n",
        "            src = layer(src)\n",
        "            \n",
        "        #src = [batch size, src len, hid dim]\n",
        "            \n",
        "        return src\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout,device \n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout,device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.MLP1=nn.Linear(hid_dim,pf_dim)\n",
        "        self.MLP2=nn.Linear(pf_dim,hid_dim)\n",
        "        self.tanh_layer=nn.Tanh()\n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len] \n",
        "                \n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src) #_src H  src H\n",
        "        \n",
        "        #dropout, residual connection and layer norm\n",
        "        t=self.MLP1(src+_src)\n",
        "        t = torch.nn.functional.relu(t)\n",
        "        t=self.dropout(t)\n",
        "        t=self.MLP2(t)\n",
        "        t=torch.nn.functional.relu(t)\n",
        "        t=self.dropout(t)\n",
        "        \n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        #positionwise feedforward\n",
        "        src = self.tanh_layer(t+src+_src) #\n",
        "        \n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src)\n",
        "        \n",
        "        #src = [batch size, src len, hid dim]\n",
        "        \n",
        "        return src\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout,device):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert hid_dim % n_heads == 0\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        #self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        \n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        \n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "                \n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "                \n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "                \n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        \n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "        \n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        x = self.fc_o(x)\n",
        "        \n",
        "        #x = [batch size, query len, hid dim]\n",
        "        \n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        new_x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        \n",
        "        #x = [batch size, seq len, pf dim]\n",
        "        \n",
        "        new_x = self.fc_2(new_x)\n",
        "        \n",
        "        #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "Y_ixwlbdZgG9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hos5_L8i6O4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "#####################\n",
        "\n",
        "#the window size in {10, 15}, the market context weight\n",
        "#  in {0.01, 0.1, 1}, the hidden layer size  in {64, 128}, the number\n",
        "# of epochs in {100, 200}, and the learning rate in {0.001, 0.0001}. We\n",
        "# set the strength  of selective regularization to 1 and the dropout\n",
        "# rate to 0.15. We use the Adam optimizer [14] for the training with\n",
        "# the early stopping by the validation accuracy. For competitors, we\n",
        "# use the default settings in their public implementations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Here we define our model as a class\n",
        "class DTML(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim,encoder):\n",
        "        super(DTML, self).__init__()\n",
        "        self.softmax_layer=nn.Softmax(dim=1)\n",
        "        #feature transformerlayer\n",
        "        self.feature_transformer=nn.Linear(input_dim,hidden_dim)\n",
        "        self.tanh_layer=nn.Tanh()\n",
        "        #self.global_train=global_train\n",
        "\n",
        "        self.global_feature_transformer=nn.Linear(input_dim,hidden_dim)\n",
        "        self.global_tanh_layer=nn.Tanh()\n",
        "\n",
        "        self.layer_nomalization= nn.LayerNorm(hidden_dim)\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.stocklstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.globallstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "        #transformer\n",
        "        self.encoder=encoder\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x,numbers,global_train,beta):\n",
        "        #print(numbers)\n",
        "        #input()\n",
        "        # Initialize hidden state with zeros\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x=self.feature_transformer(x)\n",
        "        #input()\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        x=self.tanh_layer(x)\n",
        "\n",
        "        global_train=self.global_feature_transformer(global_train)\n",
        "        #print(global_train)\n",
        "        #print(global_train.shape)\n",
        "        global_train=self.global_tanh_layer(global_train)\n",
        "        \n",
        "        #print(x)\n",
        "        #print(x.shape) #torch.Size([98750, 10, 11])\n",
        "        #input()\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "\n",
        "        #input()\n",
        "        global_h0 = torch.zeros(self.num_layers, global_train.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "        global_c0 = torch.zeros(self.num_layers, global_train.size(0), self.hidden_dim).requires_grad_().to(device)\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        #print(x)\n",
        "        #print(x.shape)\n",
        "        out, (hn, cn) = self.stocklstm(x, (h0.detach(), c0.detach()))\n",
        "        #input()\n",
        "        global_out, (global_hn,global_cn) = self.globallstm(global_train,(global_h0.detach(), global_c0.detach()))\n",
        "\n",
        "        #print(global_out.shape) #torch.Size([1975, 10, 32])\n",
        "\n",
        "        #global market attention\n",
        "        global_base=global_out[:,-1,:].view(-1,1,self.hidden_dim)\n",
        "        global_base=global_base.transpose(1,2)\n",
        "        global_score=torch.bmm(global_out,global_base)\n",
        "        global_score = self.softmax_layer(global_score) #softmax score\n",
        "        global_transpose_out=global_out[:,:,:].transpose(1,2) #torch.Size([98750, 32, 10])\n",
        "        global_new_out=torch.bmm(global_transpose_out,global_score)\n",
        "        global_new_out=global_new_out.view(-1,self.hidden_dim)\n",
        "        #print(global_new_out)\n",
        "        #print(global_new_out.shape) #torch.Size([1975, 32])\n",
        "        #print(len(global_new_out))\n",
        "        \n",
        "\n",
        "\n",
        "        sumscore=0\n",
        "        scorelist=[]\n",
        "        #print(out[:,-1,:].shape)#torch.Size([98750, 32])\n",
        "        #print(out[:,:,:].shape) #torch.Size([98750, 10, 32])\n",
        "        \n",
        "\n",
        "        #stock attention\n",
        "        base=out[:,-1,:].view(-1,1,self.hidden_dim)\n",
        "        base=base.transpose(1,2)\n",
        "        score=torch.bmm(out,base)\n",
        "        score = self.softmax_layer(score) #softmax score\n",
        "        transpose_out=out[:,:,:].transpose(1,2) #torch.Size([98750, 32, 10])\n",
        "        new_out=torch.bmm(transpose_out,score)\n",
        "        new_out=new_out.view(-1,self.hidden_dim)\n",
        "        #print(new_out)\n",
        "        #print(new_out.shape) #torch.Size([98750, 32])\n",
        "        #new_out=self.layer_nomalization(new_out)\n",
        "        myindex=0\n",
        "        for i in range(85):  #ACL 85, KDD 50\n",
        "          for j in range(len(global_new_out)):\n",
        "            \n",
        "            \n",
        "            new_out[myindex]=new_out[myindex]+global_new_out[j]*beta\n",
        "            \n",
        "        \n",
        "        #print(new_out.shape) #torch.Size([98750, 32]) H: 50,32  W size:32x32  H layer 3  Q,K,V  50x32 \n",
        "        # 98750,50,32 \n",
        "        #print(new_out[0].shape)\n",
        "        temp1=new_out[0]\n",
        "        temp2=new_out[1]\n",
        "        #print(temp1,temp2)\n",
        "        temp3=torch.stack([temp1,temp2],0)\n",
        "        #print(temp3)\n",
        "        #print(temp3.shape)  #1975,50,32  \n",
        "        new_data_list=[]\n",
        "        #print(len(global_new_out))\n",
        "        #input()\n",
        "        for i in range(len(global_new_out)):\n",
        "          temp=[]\n",
        "          for j in range(85): #ACL 85, KDD 50\n",
        "            temp.append(new_out[i+numbers*j])\n",
        "          ttt=torch.stack(temp,0)  #1,50,32\n",
        "          new_data_list.append(ttt)\n",
        "          #Q K  50x32 32x50 -> 50x50 attention map  attention map softmax   value  50x32 query attention    prediction\n",
        "        #print(len(new_data_list))\n",
        "        transformerdatalist=torch.stack(new_data_list,0)  #1,50,32\n",
        "        #print(transformerdatalist.shape)\n",
        "\n",
        "        my_output=encoder(transformerdatalist)\n",
        "        #print(my_output)\n",
        "        #print(my_output.shape) #1975,50,32\n",
        "        result_list=[]\n",
        "        for i in range(85):\n",
        "          for j in range(len(global_new_out)):\n",
        "            result_list.append(my_output[j][i])\n",
        "        #print(len(result_list))\n",
        "        #print(len(result_list[0]))\n",
        "        new_new_out=torch.stack(result_list,0)\n",
        "        #print(new_new_out.shape)\n",
        "        \n",
        "        out = self.fc(new_new_out)\n",
        "        \n",
        "        #input() \n",
        "        # out.size() --> 100, 10\n",
        "        return out\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c5dbpw7-s8lV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train)//85)\n",
        "print(len(x_val)//85)\n",
        "print(len(x_test)//85)\n",
        "encoder=Encoder(\n",
        "                 input_dim, \n",
        "                 hidden_dim, \n",
        "                 1, #encoder layer  \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device, \n",
        "                 \n",
        "                 max_length = 100)\n",
        "model = DTML(input_dim=input_dim, hidden_dim=hidden, output_dim=output_dim, num_layers=num_layers,encoder=encoder).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yX2o6EWtFlP",
        "outputId": "bfd59cd0-b385-4934-fa05-18498fd40e5b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "398\n",
            "42\n",
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (name, param) in enumerate(model.named_parameters()):\n",
        "  print(idx, name, param.requires_grad)\n",
        "  print(model.parameters())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LiY_-cuDEWv",
        "outputId": "50e6e741-106c-458c-de09-4357636a01bf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 feature_transformer.weight True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "1 feature_transformer.bias True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "2 global_feature_transformer.weight True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "3 global_feature_transformer.bias True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "4 layer_nomalization.weight True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "5 layer_nomalization.bias True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "6 stocklstm.weight_ih_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "7 stocklstm.weight_hh_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "8 stocklstm.bias_ih_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "9 stocklstm.bias_hh_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "10 globallstm.weight_ih_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb41329d0>\n",
            "11 globallstm.weight_hh_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb4132550>\n",
            "12 globallstm.bias_ih_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb4132550>\n",
            "13 globallstm.bias_hh_l0 True\n",
            "<generator object Module.parameters at 0x7f7bb4132550>\n",
            "14 encoder.layers.0.self_attn_layer_norm.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132950>\n",
            "15 encoder.layers.0.self_attn_layer_norm.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132950>\n",
            "16 encoder.layers.0.ff_layer_norm.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132950>\n",
            "17 encoder.layers.0.ff_layer_norm.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132950>\n",
            "18 encoder.layers.0.self_attention.fc_q.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "19 encoder.layers.0.self_attention.fc_q.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "20 encoder.layers.0.self_attention.fc_k.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "21 encoder.layers.0.self_attention.fc_k.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "22 encoder.layers.0.self_attention.fc_v.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "23 encoder.layers.0.self_attention.fc_v.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "24 encoder.layers.0.self_attention.fc_o.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "25 encoder.layers.0.self_attention.fc_o.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "26 encoder.layers.0.positionwise_feedforward.fc_1.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "27 encoder.layers.0.positionwise_feedforward.fc_1.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "28 encoder.layers.0.positionwise_feedforward.fc_2.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "29 encoder.layers.0.positionwise_feedforward.fc_2.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4132850>\n",
            "30 encoder.layers.0.MLP1.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4132950>\n",
            "31 encoder.layers.0.MLP1.bias True\n",
            "<generator object Module.parameters at 0x7f7bb49d4ed0>\n",
            "32 encoder.layers.0.MLP2.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4143950>\n",
            "33 encoder.layers.0.MLP2.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4143950>\n",
            "34 fc.weight True\n",
            "<generator object Module.parameters at 0x7f7bb4143950>\n",
            "35 fc.bias True\n",
            "<generator object Module.parameters at 0x7f7bb4143950>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "#####################\n",
        "num_epochs = 400\n",
        "hist = np.zeros(num_epochs)\n",
        "seq=10\n",
        "# Number of steps to unroll\n",
        "seq_dim =seq-1  \n",
        "beta=1\n",
        "patience=100\n",
        "check_early_stopping=0\n",
        "val_accuracy=0\n",
        "test_accuracy=0\n",
        "# Build model\n",
        "#####################\n",
        "\n",
        "#the window size in {10, 15}, the market context weight\n",
        "#  in {0.01, 0.1, 1}, the hidden layer size  in {64, 128}, the number\n",
        "# of epochs in {100, 200}, and the learning rate in {0.001, 0.0001}. We\n",
        "# set the strength  of selective regularization to 1 and the dropout\n",
        "# rate to 0.15. We use the Adam optimizer [14] for the training with\n",
        "# the early stopping by the validation accuracy. For competitors, we\n",
        "# use the default settings in their public implementations.\n",
        "input_dim = 11\n",
        "\n",
        "num_layers = 1 \n",
        "output_dim = 1\n",
        "stocknumbers=85 #stocknumbers acl 85. kdd 50\n",
        "n_heads=4\n",
        "\n",
        "dropout=0.15\n",
        "\n",
        "beta_dir=[1] #1\n",
        "hidden_list=[128 ] #128\n",
        "lr_list=[0.001] #0.0001\n",
        "\n",
        "result_dir={}\n",
        "\n",
        "for beta in beta_dir:\n",
        "  for hidden in hidden_list:\n",
        "    for learningrate in lr_list:\n",
        "      hidden_dim=hidden\n",
        "      pf_dim=hidden_dim * 4\n",
        "      encoder=Encoder(\n",
        "                 input_dim, \n",
        "                 hidden_dim, \n",
        "                 1, #encoder layer  \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device, \n",
        "                 \n",
        "                 max_length = 100)\n",
        "      model = DTML(input_dim=input_dim, hidden_dim=hidden, output_dim=output_dim, num_layers=num_layers,encoder=encoder).to(device)\n",
        "\n",
        "      loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "      optimiser = torch.optim.Adam(model.parameters(), lr=learningrate)\n",
        "      print(beta,hidden,learningrate)\n",
        "\n",
        "\n",
        "\n",
        "      for t in range(num_epochs):\n",
        "          # Initialise hidden state\n",
        "          # Don't do this if you want your LSTM to be stateful\n",
        "          #model.hidden = model.init_hidden()\n",
        "          model.train()\n",
        "          # Forward pass\n",
        "          y_train_pred = model(x_train,len(x_train)//85,global_train,beta)\n",
        "          # print(y_train_pred)\n",
        "          # print(y_train_pred.shape)\n",
        "          # print(y_train)\n",
        "          # print(y_train.shape)\n",
        "          print(\"traindata accuracy \",get_accuracy(y_train,y_train_pred))\n",
        "          loss = loss_fn(y_train_pred, y_train)\n",
        "          l2_lambda = 1\n",
        "          # l2_norm = sum(p.pow(2.0).sum()\n",
        "          #               for p in model.parameters())\n",
        "          for idx, p in enumerate(model.parameters()):\n",
        "            if idx==34:\n",
        "              l2_norm = p.pow(2.0).sum()\n",
        "          loss = loss + l2_lambda * l2_norm\n",
        "          #print(loss)\n",
        "          \n",
        "          if t % 10 == 0 and t !=0:\n",
        "              print(\"Epoch \", t, \"CE: \", loss.item())\n",
        "          hist[t] = loss.item()\n",
        "          print(\"Epoch: \",t)\n",
        "          print(\"training acc: {}, training loss: {}\".format(get_accuracy(y_train,y_train_pred),loss))\n",
        "          \n",
        "          # Zero out gradient, else they will accumulate between epochs\n",
        "          optimiser.zero_grad()\n",
        "\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "\n",
        "          # Update parameters\n",
        "          optimiser.step()\n",
        "\n",
        "          model.eval()\n",
        "          correct=0\n",
        "          with torch.no_grad():\n",
        "            y_val_pred=model(x_val,len(x_val)//85,global_val,beta)\n",
        "            val_acc=get_accuracy(y_val,y_val_pred)\n",
        "\n",
        "            y_test_pred=model(x_test,len(x_test)//85,global_test,beta)\n",
        "            test_acc=get_accuracy(y_test,y_test_pred)\n",
        "            #print(y_val_pred)\n",
        "            #print(get_accuracy(y_val,y_val_pred))\n",
        "            if val_accuracy <val_acc:\n",
        "              val_accuracy=val_acc\n",
        "              test_accuracy=test_acc\n",
        "              check_early_stopping=0\n",
        "            else:\n",
        "              check_early_stopping=check_early_stopping+1\n",
        "          print(\"val acc: {}, test acc: {}\".format(val_acc,test_acc))\n",
        "          if check_early_stopping==patience:\n",
        "            print(\"early stopping!\")\n",
        "            print(val_accuracy)\n",
        "            print(\"epochs: \",t)\n",
        "            break\n",
        "      #test score\n",
        "      # model.eval()\n",
        "      # with torch.no_grad():\n",
        "      #     y_test_pred=model(x_test)\n",
        "      #     print(get_accuracy(y_test,y_test_pred))\n",
        "      print(\"Best test acc: \",test_accuracy)\n",
        "      result_dir[\"beta_{}_hidden_{}_lr_{}\".format(beta,hidden,learningrate)]=get_accuracy(y_test,y_test_pred)\n",
        "      torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4S3RJRQQtOb",
        "outputId": "efbcce16-d7b2-4ee8-b0a4-4884c9f0fbc9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 128 0.001\n",
            "traindata accuracy  0.5079515223174697\n",
            "Epoch:  0\n",
            "training acc: 0.5079515223174697, training loss: 1.023621916770935\n",
            "val acc: 0.5056022408963585, test acc: 0.48216911764705883\n",
            "traindata accuracy  0.4895654744309784\n",
            "Epoch:  1\n",
            "training acc: 0.4895654744309784, training loss: 1.145188331604004\n",
            "val acc: 0.503921568627451, test acc: 0.4786764705882353\n",
            "traindata accuracy  0.492403192432752\n",
            "Epoch:  2\n",
            "training acc: 0.492403192432752, training loss: 1.014143466949463\n",
            "val acc: 0.503641456582633, test acc: 0.5056985294117647\n",
            "traindata accuracy  0.5115282293822051\n",
            "Epoch:  3\n",
            "training acc: 0.5115282293822051, training loss: 0.9914840459823608\n",
            "val acc: 0.49831932773109244, test acc: 0.5071691176470589\n",
            "traindata accuracy  0.5115873485072421\n",
            "Epoch:  4\n",
            "training acc: 0.5115873485072421, training loss: 1.0111513137817383\n",
            "val acc: 0.4969187675070028, test acc: 0.5068014705882353\n",
            "traindata accuracy  0.5111143955069465\n",
            "Epoch:  5\n",
            "training acc: 0.5111143955069465, training loss: 0.9988420605659485\n",
            "val acc: 0.49159663865546216, test acc: 0.5064338235294118\n",
            "traindata accuracy  0.5126810523204257\n",
            "Epoch:  6\n",
            "training acc: 0.5126810523204257, training loss: 0.9727261066436768\n",
            "val acc: 0.4910364145658263, test acc: 0.5066176470588235\n",
            "traindata accuracy  0.5124445758202779\n",
            "Epoch:  7\n",
            "training acc: 0.5124445758202779, training loss: 0.9512336254119873\n",
            "val acc: 0.49411764705882355, test acc: 0.525\n",
            "traindata accuracy  0.5127697310079811\n",
            "Epoch:  8\n",
            "training acc: 0.5127697310079811, training loss: 0.9407615661621094\n",
            "val acc: 0.5182072829131653, test acc: 0.4904411764705882\n",
            "traindata accuracy  0.4880579367425362\n",
            "Epoch:  9\n",
            "training acc: 0.4880579367425362, training loss: 0.9363691806793213\n",
            "val acc: 0.5095238095238095, test acc: 0.48455882352941176\n",
            "traindata accuracy  0.48696423292935265\n",
            "Epoch  10 CE:  0.932701587677002\n",
            "Epoch:  10\n",
            "training acc: 0.48696423292935265, training loss: 0.932701587677002\n",
            "val acc: 0.5016806722689076, test acc: 0.4832720588235294\n",
            "traindata accuracy  0.4891516405557198\n",
            "Epoch:  11\n",
            "training acc: 0.4891516405557198, training loss: 0.9263142347335815\n",
            "val acc: 0.49831932773109244, test acc: 0.484375\n",
            "traindata accuracy  0.48989062961868163\n",
            "Epoch:  12\n",
            "training acc: 0.48989062961868163, training loss: 0.9178380966186523\n",
            "val acc: 0.49607843137254903, test acc: 0.48566176470588235\n",
            "traindata accuracy  0.4902453443689033\n",
            "Epoch:  13\n",
            "training acc: 0.4902453443689033, training loss: 0.9084266424179077\n",
            "val acc: 0.4991596638655462, test acc: 0.48768382352941175\n",
            "traindata accuracy  0.4907774164942359\n",
            "Epoch:  14\n",
            "training acc: 0.4907774164942359, training loss: 0.8987221717834473\n",
            "val acc: 0.5008403361344538, test acc: 0.5110294117647058\n",
            "traindata accuracy  0.4997930830623707\n",
            "Epoch:  15\n",
            "training acc: 0.4997930830623707, training loss: 0.8904486298561096\n",
            "val acc: 0.4890756302521008, test acc: 0.5240808823529411\n",
            "traindata accuracy  0.515193615134496\n",
            "Epoch:  16\n",
            "training acc: 0.515193615134496, training loss: 0.8827529549598694\n",
            "val acc: 0.48291316526610645, test acc: 0.5121323529411764\n",
            "traindata accuracy  0.5159917233224949\n",
            "Epoch:  17\n",
            "training acc: 0.5159917233224949, training loss: 0.8762040138244629\n",
            "val acc: 0.4823529411764706, test acc: 0.5084558823529411\n",
            "traindata accuracy  0.5158734850724209\n",
            "Epoch:  18\n",
            "training acc: 0.5158734850724209, training loss: 0.8701084852218628\n",
            "val acc: 0.48095238095238096, test acc: 0.50625\n",
            "traindata accuracy  0.5144546260715341\n",
            "Epoch:  19\n",
            "training acc: 0.5144546260715341, training loss: 0.8643507957458496\n",
            "val acc: 0.48207282913165267, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.5155483298847177\n",
            "Epoch  20 CE:  0.8586888313293457\n",
            "Epoch:  20\n",
            "training acc: 0.5155483298847177, training loss: 0.8586888313293457\n",
            "val acc: 0.4823529411764706, test acc: 0.5091911764705882\n",
            "traindata accuracy  0.5135973987584984\n",
            "Epoch:  21\n",
            "training acc: 0.5135973987584984, training loss: 0.8527069091796875\n",
            "val acc: 0.48627450980392156, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.5135382796334614\n",
            "Epoch:  22\n",
            "training acc: 0.5135382796334614, training loss: 0.8469774723052979\n",
            "val acc: 0.48739495798319327, test acc: 0.5080882352941176\n",
            "traindata accuracy  0.5125628140703518\n",
            "Epoch:  23\n",
            "training acc: 0.5125628140703518, training loss: 0.8411375284194946\n",
            "val acc: 0.48851540616246497, test acc: 0.5091911764705882\n",
            "traindata accuracy  0.5142477091339048\n",
            "Epoch:  24\n",
            "training acc: 0.5142477091339048, training loss: 0.8353014588356018\n",
            "val acc: 0.48711484593837534, test acc: 0.5091911764705882\n",
            "traindata accuracy  0.5149275790718297\n",
            "Epoch:  25\n",
            "training acc: 0.5149275790718297, training loss: 0.8296493291854858\n",
            "val acc: 0.48403361344537815, test acc: 0.5084558823529411\n",
            "traindata accuracy  0.515252734259533\n",
            "Epoch:  26\n",
            "training acc: 0.515252734259533, training loss: 0.8242515921592712\n",
            "val acc: 0.47955182072829133, test acc: 0.5038602941176471\n",
            "traindata accuracy  0.516435116760272\n",
            "Epoch:  27\n",
            "training acc: 0.516435116760272, training loss: 0.819162130355835\n",
            "val acc: 0.4764705882352941, test acc: 0.5034926470588236\n",
            "traindata accuracy  0.5206325746378954\n",
            "Epoch:  28\n",
            "training acc: 0.5206325746378954, training loss: 0.8141466975212097\n",
            "val acc: 0.473109243697479, test acc: 0.5071691176470589\n",
            "traindata accuracy  0.5195093112621934\n",
            "Epoch:  29\n",
            "training acc: 0.5195093112621934, training loss: 0.8095253705978394\n",
            "val acc: 0.4711484593837535, test acc: 0.5080882352941176\n",
            "traindata accuracy  0.518120011823825\n",
            "Epoch  30 CE:  0.8051394820213318\n",
            "Epoch:  30\n",
            "training acc: 0.518120011823825, training loss: 0.8051394820213318\n",
            "val acc: 0.47058823529411764, test acc: 0.5097426470588236\n",
            "traindata accuracy  0.5190954773869346\n",
            "Epoch:  31\n",
            "training acc: 0.5190954773869346, training loss: 0.8006871938705444\n",
            "val acc: 0.4781512605042017, test acc: 0.5101102941176471\n",
            "traindata accuracy  0.519923145137452\n",
            "Epoch:  32\n",
            "training acc: 0.519923145137452, training loss: 0.7964890599250793\n",
            "val acc: 0.4823529411764706, test acc: 0.5152573529411765\n",
            "traindata accuracy  0.5189772391368608\n",
            "Epoch:  33\n",
            "training acc: 0.5189772391368608, training loss: 0.7924977540969849\n",
            "val acc: 0.48599439775910364, test acc: 0.5178308823529412\n",
            "traindata accuracy  0.5192728347620456\n",
            "Epoch:  34\n",
            "training acc: 0.5192728347620456, training loss: 0.7884127497673035\n",
            "val acc: 0.4831932773109244, test acc: 0.5152573529411765\n",
            "traindata accuracy  0.5207803724504877\n",
            "Epoch:  35\n",
            "training acc: 0.5207803724504877, training loss: 0.7846799492835999\n",
            "val acc: 0.47955182072829133, test acc: 0.515625\n",
            "traindata accuracy  0.5215193615134496\n",
            "Epoch:  36\n",
            "training acc: 0.5215193615134496, training loss: 0.780787467956543\n",
            "val acc: 0.47955182072829133, test acc: 0.5152573529411765\n",
            "traindata accuracy  0.5225243866390777\n",
            "Epoch:  37\n",
            "training acc: 0.5225243866390777, training loss: 0.7771469950675964\n",
            "val acc: 0.48291316526610645, test acc: 0.5108455882352941\n",
            "traindata accuracy  0.519302394324564\n",
            "Epoch:  38\n",
            "training acc: 0.519302394324564, training loss: 0.7736697196960449\n",
            "val acc: 0.4834733893557423, test acc: 0.5139705882352941\n",
            "traindata accuracy  0.5207508128879692\n",
            "Epoch:  39\n",
            "training acc: 0.5207508128879692, training loss: 0.7702189087867737\n",
            "val acc: 0.48487394957983193, test acc: 0.5126838235294118\n",
            "traindata accuracy  0.5214011232633757\n",
            "Epoch  40 CE:  0.7669179439544678\n",
            "Epoch:  40\n",
            "training acc: 0.5214011232633757, training loss: 0.7669179439544678\n",
            "val acc: 0.48851540616246497, test acc: 0.5090073529411765\n",
            "traindata accuracy  0.5200709429500443\n",
            "Epoch:  41\n",
            "training acc: 0.5200709429500443, training loss: 0.7638512849807739\n",
            "val acc: 0.48935574229691875, test acc: 0.5104779411764706\n",
            "traindata accuracy  0.5201596216375998\n",
            "Epoch:  42\n",
            "training acc: 0.5201596216375998, training loss: 0.7607619762420654\n",
            "val acc: 0.49131652661064423, test acc: 0.5079044117647059\n",
            "traindata accuracy  0.5197753473248596\n",
            "Epoch:  43\n",
            "training acc: 0.5197753473248596, training loss: 0.7578882575035095\n",
            "val acc: 0.49131652661064423, test acc: 0.5095588235294117\n",
            "traindata accuracy  0.5196275495122672\n",
            "Epoch:  44\n",
            "training acc: 0.5196275495122672, training loss: 0.7551429271697998\n",
            "val acc: 0.4935574229691877, test acc: 0.5102941176470588\n",
            "traindata accuracy  0.5179722140112326\n",
            "Epoch:  45\n",
            "training acc: 0.5179722140112326, training loss: 0.7523730397224426\n",
            "val acc: 0.4935574229691877, test acc: 0.5084558823529411\n",
            "traindata accuracy  0.5171445462607154\n",
            "Epoch:  46\n",
            "training acc: 0.5171445462607154, training loss: 0.7498297691345215\n",
            "val acc: 0.49047619047619045, test acc: 0.5080882352941176\n",
            "traindata accuracy  0.5179722140112326\n",
            "Epoch:  47\n",
            "training acc: 0.5179722140112326, training loss: 0.7472801804542542\n",
            "val acc: 0.4910364145658263, test acc: 0.5090073529411765\n",
            "traindata accuracy  0.5189476795743423\n",
            "Epoch:  48\n",
            "training acc: 0.5189476795743423, training loss: 0.7448440194129944\n",
            "val acc: 0.4907563025210084, test acc: 0.508639705882353\n",
            "traindata accuracy  0.518681643511676\n",
            "Epoch:  49\n",
            "training acc: 0.518681643511676, training loss: 0.742557168006897\n",
            "val acc: 0.49131652661064423, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5192432751995271\n",
            "Epoch  50 CE:  0.7402309775352478\n",
            "Epoch:  50\n",
            "training acc: 0.5192432751995271, training loss: 0.7402309775352478\n",
            "val acc: 0.4899159663865546, test acc: 0.5079044117647059\n",
            "traindata accuracy  0.518120011823825\n",
            "Epoch:  51\n",
            "training acc: 0.518120011823825, training loss: 0.7380802035331726\n",
            "val acc: 0.48935574229691875, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.5195684303872302\n",
            "Epoch:  52\n",
            "training acc: 0.5195684303872302, training loss: 0.7360683083534241\n",
            "val acc: 0.4899159663865546, test acc: 0.5071691176470589\n",
            "traindata accuracy  0.5184451670115282\n",
            "Epoch:  53\n",
            "training acc: 0.5184451670115282, training loss: 0.7340599298477173\n",
            "val acc: 0.49019607843137253, test acc: 0.5075367647058824\n",
            "traindata accuracy  0.5200709429500443\n",
            "Epoch:  54\n",
            "training acc: 0.5200709429500443, training loss: 0.7322161793708801\n",
            "val acc: 0.4910364145658263, test acc: 0.5079044117647059\n",
            "traindata accuracy  0.519864026012415\n",
            "Epoch:  55\n",
            "training acc: 0.519864026012415, training loss: 0.7303005456924438\n",
            "val acc: 0.49019607843137253, test acc: 0.5077205882352941\n",
            "traindata accuracy  0.5168193910730121\n",
            "Epoch:  56\n",
            "training acc: 0.5168193910730121, training loss: 0.7286419868469238\n",
            "val acc: 0.49131652661064423, test acc: 0.5088235294117647\n",
            "traindata accuracy  0.5203369790127106\n",
            "Epoch:  57\n",
            "training acc: 0.5203369790127106, training loss: 0.7268784642219543\n",
            "val acc: 0.49131652661064423, test acc: 0.5090073529411765\n",
            "traindata accuracy  0.519923145137452\n",
            "Epoch:  58\n",
            "training acc: 0.519923145137452, training loss: 0.7252697944641113\n",
            "val acc: 0.4918767507002801, test acc: 0.5082720588235294\n",
            "traindata accuracy  0.519982264262489\n",
            "Epoch:  59\n",
            "training acc: 0.519982264262489, training loss: 0.723763644695282\n",
            "val acc: 0.4899159663865546, test acc: 0.5077205882352941\n",
            "traindata accuracy  0.5180904522613066\n",
            "Epoch  60 CE:  0.722291111946106\n",
            "Epoch:  60\n",
            "training acc: 0.5180904522613066, training loss: 0.722291111946106\n",
            "val acc: 0.48851540616246497, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5180017735737511\n",
            "Epoch:  61\n",
            "training acc: 0.5180017735737511, training loss: 0.7208585143089294\n",
            "val acc: 0.48739495798319327, test acc: 0.5080882352941176\n",
            "traindata accuracy  0.5190363582618978\n",
            "Epoch:  62\n",
            "training acc: 0.5190363582618978, training loss: 0.7194519639015198\n",
            "val acc: 0.48851540616246497, test acc: 0.5075367647058824\n",
            "traindata accuracy  0.5210168489506355\n",
            "Epoch:  63\n",
            "training acc: 0.5210168489506355, training loss: 0.7181164622306824\n",
            "val acc: 0.4857142857142857, test acc: 0.5079044117647059\n",
            "traindata accuracy  0.5207212533254508\n",
            "Epoch:  64\n",
            "training acc: 0.5207212533254508, training loss: 0.7169537544250488\n",
            "val acc: 0.4857142857142857, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.5198935855749335\n",
            "Epoch:  65\n",
            "training acc: 0.5198935855749335, training loss: 0.7156989574432373\n",
            "val acc: 0.48627450980392156, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5203665385752291\n",
            "Epoch:  66\n",
            "training acc: 0.5203665385752291, training loss: 0.7145621180534363\n",
            "val acc: 0.48739495798319327, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5207508128879692\n",
            "Epoch:  67\n",
            "training acc: 0.5207508128879692, training loss: 0.7134537100791931\n",
            "val acc: 0.48711484593837534, test acc: 0.5091911764705882\n",
            "traindata accuracy  0.5203665385752291\n",
            "Epoch:  68\n",
            "training acc: 0.5203665385752291, training loss: 0.7124430537223816\n",
            "val acc: 0.48515406162464986, test acc: 0.5090073529411765\n",
            "traindata accuracy  0.522731303576707\n",
            "Epoch:  69\n",
            "training acc: 0.522731303576707, training loss: 0.7114375829696655\n",
            "val acc: 0.48599439775910364, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5193319538870824\n",
            "Epoch  70 CE:  0.7105118036270142\n",
            "Epoch:  70\n",
            "training acc: 0.5193319538870824, training loss: 0.7105118036270142\n",
            "val acc: 0.48739495798319327, test acc: 0.5079044117647059\n",
            "traindata accuracy  0.521046408513154\n",
            "Epoch:  71\n",
            "training acc: 0.521046408513154, training loss: 0.7095741033554077\n",
            "val acc: 0.48739495798319327, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5193319538870824\n",
            "Epoch:  72\n",
            "training acc: 0.5193319538870824, training loss: 0.7086501717567444\n",
            "val acc: 0.4887955182072829, test acc: 0.508639705882353\n",
            "traindata accuracy  0.5183564883239729\n",
            "Epoch:  73\n",
            "training acc: 0.5183564883239729, training loss: 0.7078278064727783\n",
            "val acc: 0.48851540616246497, test acc: 0.5080882352941176\n",
            "traindata accuracy  0.5207212533254508\n",
            "Epoch:  74\n",
            "training acc: 0.5207212533254508, training loss: 0.7070630788803101\n",
            "val acc: 0.4890756302521008, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.5200413833875258\n",
            "Epoch:  75\n",
            "training acc: 0.5200413833875258, training loss: 0.706258237361908\n",
            "val acc: 0.48823529411764705, test acc: 0.5077205882352941\n",
            "traindata accuracy  0.5213420041383388\n",
            "Epoch:  76\n",
            "training acc: 0.5213420041383388, training loss: 0.7055548429489136\n",
            "val acc: 0.48627450980392156, test acc: 0.506985294117647\n",
            "traindata accuracy  0.5200118238250074\n",
            "Epoch:  77\n",
            "training acc: 0.5200118238250074, training loss: 0.704865038394928\n",
            "val acc: 0.48627450980392156, test acc: 0.5068014705882353\n",
            "traindata accuracy  0.5206325746378954\n",
            "Epoch:  78\n",
            "training acc: 0.5206325746378954, training loss: 0.7041791677474976\n",
            "val acc: 0.4868347338935574, test acc: 0.5068014705882353\n",
            "traindata accuracy  0.5214898019509311\n",
            "Epoch:  79\n",
            "training acc: 0.5214898019509311, training loss: 0.703549325466156\n",
            "val acc: 0.4857142857142857, test acc: 0.50625\n",
            "traindata accuracy  0.519804906887378\n",
            "Epoch  80 CE:  0.7030160427093506\n",
            "Epoch:  80\n",
            "training acc: 0.519804906887378, training loss: 0.7030160427093506\n",
            "val acc: 0.4857142857142857, test acc: 0.5064338235294118\n",
            "traindata accuracy  0.5211942063257464\n",
            "Epoch:  81\n",
            "training acc: 0.5211942063257464, training loss: 0.7024039626121521\n",
            "val acc: 0.48599439775910364, test acc: 0.506985294117647\n",
            "traindata accuracy  0.5188294413242684\n",
            "Epoch:  82\n",
            "training acc: 0.5188294413242684, training loss: 0.7018681168556213\n",
            "val acc: 0.48627450980392156, test acc: 0.5071691176470589\n",
            "traindata accuracy  0.5203074194501921\n",
            "Epoch:  83\n",
            "training acc: 0.5203074194501921, training loss: 0.7013221383094788\n",
            "val acc: 0.4854341736694678, test acc: 0.506985294117647\n",
            "traindata accuracy  0.5205734555128584\n",
            "Epoch:  84\n",
            "training acc: 0.5205734555128584, training loss: 0.7008483409881592\n",
            "val acc: 0.4857142857142857, test acc: 0.506985294117647\n",
            "traindata accuracy  0.5187112030741945\n",
            "Epoch:  85\n",
            "training acc: 0.5187112030741945, training loss: 0.7004022598266602\n",
            "val acc: 0.4857142857142857, test acc: 0.5071691176470589\n",
            "traindata accuracy  0.5192137156370086\n",
            "Epoch:  86\n",
            "training acc: 0.5192137156370086, training loss: 0.6999401450157166\n",
            "val acc: 0.4857142857142857, test acc: 0.506985294117647\n",
            "traindata accuracy  0.5191250369494531\n",
            "Epoch:  87\n",
            "training acc: 0.5191250369494531, training loss: 0.6995363831520081\n",
            "val acc: 0.4857142857142857, test acc: 0.5066176470588235\n",
            "traindata accuracy  0.5200413833875258\n",
            "Epoch:  88\n",
            "training acc: 0.5200413833875258, training loss: 0.6991141438484192\n",
            "val acc: 0.4857142857142857, test acc: 0.5066176470588235\n",
            "traindata accuracy  0.5201005025125628\n",
            "Epoch:  89\n",
            "training acc: 0.5201005025125628, training loss: 0.6987048983573914\n",
            "val acc: 0.4854341736694678, test acc: 0.5064338235294118\n",
            "traindata accuracy  0.5193910730121194\n",
            "Epoch  90 CE:  0.6983984112739563\n",
            "Epoch:  90\n",
            "training acc: 0.5193910730121194, training loss: 0.6983984112739563\n",
            "val acc: 0.48515406162464986, test acc: 0.5064338235294118\n",
            "traindata accuracy  0.5201596216375998\n",
            "Epoch:  91\n",
            "training acc: 0.5201596216375998, training loss: 0.6980683207511902\n",
            "val acc: 0.48515406162464986, test acc: 0.5073529411764706\n",
            "traindata accuracy  0.519923145137452\n",
            "Epoch:  92\n",
            "training acc: 0.519923145137452, training loss: 0.69771409034729\n",
            "val acc: 0.48487394957983193, test acc: 0.5068014705882353\n",
            "traindata accuracy  0.5197753473248596\n",
            "Epoch:  93\n",
            "training acc: 0.5197753473248596, training loss: 0.6974057555198669\n",
            "val acc: 0.4837535014005602, test acc: 0.5068014705882353\n",
            "traindata accuracy  0.5203960981377476\n",
            "Epoch:  94\n",
            "training acc: 0.5203960981377476, training loss: 0.6971384286880493\n",
            "val acc: 0.4834733893557423, test acc: 0.5066176470588235\n",
            "traindata accuracy  0.5189181200118238\n",
            "Epoch:  95\n",
            "training acc: 0.5189181200118238, training loss: 0.6968451738357544\n",
            "val acc: 0.4837535014005602, test acc: 0.5066176470588235\n",
            "traindata accuracy  0.5205734555128584\n",
            "Epoch:  96\n",
            "training acc: 0.5205734555128584, training loss: 0.6965844631195068\n",
            "val acc: 0.4834733893557423, test acc: 0.5064338235294118\n",
            "traindata accuracy  0.5200118238250074\n",
            "Epoch:  97\n",
            "training acc: 0.5200118238250074, training loss: 0.696333110332489\n",
            "val acc: 0.4843137254901961, test acc: 0.5060661764705883\n",
            "traindata accuracy  0.5188294413242684\n",
            "Epoch:  98\n",
            "training acc: 0.5188294413242684, training loss: 0.6961216926574707\n",
            "val acc: 0.4837535014005602, test acc: 0.5053308823529412\n",
            "traindata accuracy  0.5187703221992315\n",
            "Epoch:  99\n",
            "training acc: 0.5187703221992315, training loss: 0.6958727836608887\n",
            "val acc: 0.4831932773109244, test acc: 0.5049632352941177\n",
            "traindata accuracy  0.5207212533254508\n",
            "Epoch  100 CE:  0.6956594586372375\n",
            "Epoch:  100\n",
            "training acc: 0.5207212533254508, training loss: 0.6956594586372375\n",
            "val acc: 0.4826330532212885, test acc: 0.5042279411764706\n",
            "traindata accuracy  0.5201300620750813\n",
            "Epoch:  101\n",
            "training acc: 0.5201300620750813, training loss: 0.6954790353775024\n",
            "val acc: 0.4823529411764706, test acc: 0.5045955882352942\n",
            "traindata accuracy  0.5201596216375998\n",
            "Epoch:  102\n",
            "training acc: 0.5201596216375998, training loss: 0.6952911019325256\n",
            "val acc: 0.48207282913165267, test acc: 0.5047794117647059\n",
            "traindata accuracy  0.5198344664498965\n",
            "Epoch:  103\n",
            "training acc: 0.5198344664498965, training loss: 0.6951154470443726\n",
            "val acc: 0.48207282913165267, test acc: 0.5044117647058823\n",
            "traindata accuracy  0.5203074194501921\n",
            "Epoch:  104\n",
            "training acc: 0.5203074194501921, training loss: 0.6949381232261658\n",
            "val acc: 0.48207282913165267, test acc: 0.5045955882352942\n",
            "traindata accuracy  0.5189476795743423\n",
            "Epoch:  105\n",
            "training acc: 0.5189476795743423, training loss: 0.6947650909423828\n",
            "val acc: 0.4812324929971989, test acc: 0.5038602941176471\n",
            "traindata accuracy  0.5191841560744901\n",
            "Epoch:  106\n",
            "training acc: 0.5191841560744901, training loss: 0.6946531534194946\n",
            "val acc: 0.4812324929971989, test acc: 0.5040441176470588\n",
            "traindata accuracy  0.5204552172627845\n",
            "Epoch:  107\n",
            "training acc: 0.5204552172627845, training loss: 0.6944831013679504\n",
            "val acc: 0.48095238095238096, test acc: 0.5033088235294118\n",
            "traindata accuracy  0.5200709429500443\n",
            "Epoch:  108\n",
            "training acc: 0.5200709429500443, training loss: 0.6943790316581726\n",
            "val acc: 0.4812324929971989, test acc: 0.5027573529411765\n",
            "early stopping!\n",
            "0.5182072829131653\n",
            "epochs:  108\n",
            "Best test acc:  0.4904411764705882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xmeo0f12VCIv",
        "outputId": "7b9ae417-3bf6-4f86-b17b-8a11b77b14b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'beta_0.01_hidden_64_lr_0.001': 0.4988888888888889, 'beta_0.01_hidden_64_lr_0.0001': 0.5177777777777778, 'beta_0.01_hidden_128_lr_0.001': 0.49476190476190474, 'beta_0.01_hidden_128_lr_0.0001': 0.5284126984126984, 'beta_0.1_hidden_64_lr_0.001': 0.5308730158730158, 'beta_0.1_hidden_64_lr_0.0001': 0.4942857142857143, 'beta_0.1_hidden_128_lr_0.001': 0.5073809523809524, 'beta_0.1_hidden_128_lr_0.0001': 0.5273015873015873, 'beta_1_hidden_64_lr_0.001': 0.5311111111111111, 'beta_1_hidden_64_lr_0.0001': 0.5307142857142857, 'beta_1_hidden_128_lr_0.001': 0.5311904761904762, 'beta_1_hidden_128_lr_0.0001': 0.5053968253968254}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist, label=\"Training loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "4OeV0crXBEfv",
        "outputId": "ad5b423e-b6c1-43a7-dce0-af5ff09a4821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD3CAYAAAANMK+RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcAElEQVR4nO3de5gcdZ3v8XfPTCbJhBky0Y4BAiHcvhAu4SpmIYconEUJ6hGVdYVIlHMWiWyQLEJERJCA3DWACkc4cJDHRxSVW4QQ4TxBIAjBRTALX0ISWIiQDOskmUsuJNPnj66e7ulMZ7pnuqq7Op/XI6arurr7MzWZz/zy66quRCqVQkRE4q+u0gFERKQ8VOgiIjVChS4iUiNU6CIiNUKFLiJSIxoq9cJtbR1DOrymtbWJ9vbucsUpG+UqjXKVRrlKV63ZBpsrmWxOFLovtiP0hob6Skfol3KVRrlKo1ylq9ZsYeSKbaGLiEhfKnQRkRqhQhcRqREqdBGRGqFCFxGpESp0EZEaoUIXEakRsSz01W2d3Pvoq/Too39FRHpV7EzRobjs/zxPKgVjdx3OEfsnKx1HRIZo1qz/yaRJh7Bhw3oWL36ST3/6cwCsX7+O73zn8qKfZ+nS53nuuWc577xv7nC7O+64jQMPPIjjjz9hKLHp7u5i/vwb6enpKSlnWGJZ6JmB+abN2yobRETKYvr0zzB9+mdYufIN/vznpb2FvGDBQyU9z9FHf5SjjjpmwO3OPvscEomCZ9AXralpFCeffAqPPvrIkJ+rHGJZ6BkpNOUiUm6/evINXnhtbUmPqa9PsG1b4Z/HYw4cy+mf2K/g/dOnf6bg+p/85Gb+8IeFTJ/+GZYte4UJEyZyxBFH8fTTi9lzzwmsXPkGF144l1GjduGWW27C/TVuvfV/84tf3MNdd/2M2bNn8+KLL9HV1ck119xEW1sb8+dfz377HcDZZ5/DZZd9m9Wr3+Hww4/kzTdXMWnSwZx99jkA/Pznd/PmmyvYa6+9eeWVvzBs2DDOP/9bjBs3rt+83d1d3Hrrj9h99z147733OPbYjzF16jQWL36Sl176dz784Q/z2muvcuWV1/D444/z1FPP9lk3VLGcQ8/QFLpI7Zs1azbr1rXzxS/+M9dfP59TTjmV5uZmZs/+N2bMmInZgTz22O8B+MIXvtT7uC9/+Svsuutopk2bxuWXXwXA8uWvM27cOKZOnda73bnn/ivt7X9n1qzZXHfdD3n44QcAWLnyDRYuXMB3v3slZ511Ni0tuzJ16rSCZQ5wzz13MX78Xpx55kxmz57DTTddx4YNG3jssQUccIBxxhlncfrp/wzAAw88sN26oYr1CF1Eyu/0T+y3w9F0f5LJZtraOkJKBK2tY2hpaQFg//2N1157lbvu+hmjR4/G/TUmTtyn4GMnTpzI++93Mnp0K93dXf1us/vue1Bfn/6wrIaGdC2uWrWKPfYY32ebgaxYsZxTT/0sAI2NjTQ3N7N69ducd94F3Hvv3dx//31MmXIchxxyGN/+9reZP//WPuuGOg2kEbqIVL38orv22iuZOvUEZsz4Ksccc2xJjy3W3ntP5J133u5d/tvfVg/4mP32O4DVq98BYPPmzXR0dDB+/F6sWrWSiy++lNtvv4ulS5/n9ded5cuXb7duqGI9Qtccukjt2Lx5Ew899Ds6Ozt55JEHe0e6Dz/8AJ2dnfzyl/fypS+dCcCpp36Wu+++gyOPPBr3V+no6OCdd97moYd+y5o177FkyTNs3LiRzs5O7r//fsaNm8CKFctZuPD37LHHeJ555ik6OjpYtWolixY9xpo177F06fN0dXX1ef1//MdP8b3vXcJ++x3Ali1btvvl0N3dxcKFv2fFiuW88spfmDFjJrfc8kPuvvsO1qxZw5w5F9Hc3MyyZa+wbNkrjBgxgokT92Wfffblvvv+L88++3yfdUOVSFVomDuUC1x87ZonAfjqpw5k6uTdy5apHML+p+dgKVdplKs01ZoLhpbt1VeXcdBBBwNw9dVXMH36Z5k8+fCK5trRBS5iPUIXEQnTr3/9S/baawI9PT0kk2PLVuZhiXWha8JFRMJ02WVXVjpCSWL9pqiIiGQNOEI3s3HAPGCyuxc8BcvMTgCeAA5397+WL2JhlZr/FxGpRsWM0I8HHgQKTsSb2Vjgn4B3ypRLRERKNOAI3d3vN7Nphe43szrgauBbwCnFvnBra9OQr3q9yy4jSCabh/QcYajGTKBcpVKu0lRrLqjebOXOVY43RecCP3P3djMr+kHt7d1DfuENHZuq7lCpaj18S7lKo1ylqdZcUL3ZhnDYYsH7hvSmqJmNAA4BPm5mc4FdgbPN7MShPK+IiJRuUCN0MxsFNLl7G/DlnPVfB+6M6k1RnfsvIpI14Ag9OHplBrCbmV1qZiOBmcCVOdsMM7NLSY/Q/8XMJoWUtw/VuYhIVjFvii4GFuet/nHeNh+QPrRxXvmiDUwDdBGRLJ1YJCJSI1ToIiI1ItaFrjNFRUSy4l3olQ4gIlJFYl3oanQRkaxYF7r6XEQkK9aF3tOjShcRyYh1oeuaoiIiWbEudI3QRUSy4l3o6nMRkV6xLnQdhy4ikhXrQteUi4hIVrwLXX0uItIr1oWuKRcRkaxYF3qPCl1EpFesCz3VU+kEIiLVI9aFrhG6iEhWvAtd74qKiPQq6iLRZjaO9OXlJrv7Mf3cPxP4GLACOBK4xd2fLWPOfmmALiKSVewI/XjgQSBR4P49gG+6+/XAj4Dby5BtQJpyERHJKqrQ3f1+oGMH91/l7ptynrOzDNkGpEIXEckqasqlWGaWAM4H5gy0bWtrEw0N9UN6veHDh5FMNg/pOcJQjZlAuUqlXKWp1lxQvdnKnatshR6U+fXA3e6+ZKDt29u7h/ya3d1baGsr+A+Hikgmm6suEyhXqZSrNNWaC6o322Bz7eiXwKCPcjGzUWaWDG7XA/OBh939MTP7/GCftxSachERySr2KJcTgBnAbmZ2KXAjMBM4FPg66ZH5/wAOMzOAfYHfhJC3DxW6iEhWUYXu7ouBxXmrf5xz/xyKmDcvN/W5iEiWTiwSEakR8S50DdFFRHrFstAzZzepz0VEsmJZ6BmachERyYploScS6TG6plxERLJiWujpP1XoIiJZsSz0DPW5iEhWrAtdc+giIlmxLPTMlIsuEi0ikhXLQs/QAF1EJCuWhZ4ZmOtNURGRrFgWeqbIUxqii4j0imWha4QuIrK92BV67huhGqCLiGTFr9Bzb6vRRUR6xa/Q+4zQVegiIhkxLPTsbQ3QRUSyYljoqX5vi4js7Iq9pug4YB4w2d2P6ef+OuBqoAPYG7jT3Z8rY85euaNynfovIpJV7Aj9eOBBsteWyHc60OLuVwEXA/eYWX0Z8m1Hc+giIv0rqtDd/X7So+9CpgNLgm3/DmwCDh5yun7kdrj6XEQkq6gplyKMpW/hbwjWFdTa2kRDQ+mD+M6NH2QXEgmSyeaSnyNs1ZgJlKtUylWaas0F1Zut3LnKVehrgdxkLcG6gtrbuwf1QrmFvnXrNtradvQPh+glk81VlwmUq1TKVZpqzQXVm22wuXb0S2DQR7mY2SgzSwaLC4ApwfoxwAhg2WCfu1h6T1REJKvYo1xOAGYAu5nZpcCNwEzgUODrwK+AI8zse8BewFfcfVsYgXt02KKISL+KKnR3Xwwszlv945z7e0gf3RK6lA5bFBHpV6xPLFKfi4hkxbDQs7c1QhcRyYphoWdLfFtPTwWTiIhUl9gVeu6bolu3pfTGqIhIIHaFnt/fOv1fRCQthoXet8C3blOhi4hAHAs9b3nbNs2ji4hAHAs9r9E1QhcRSYthoedPuWiELiICMSz0/EPPt+pYdBERIIaFnj9C1xy6iEhaDAu977Lm0EVE0mJY6HkjdJ0tKiICxLLQ+y5rhC4ikha7Qs8/M1Rz6CIiabEr9HwaoYuIpMWu0DMj9EQivazj0EVE0mJX6JkZl2H16egaoYuIpMWw0NMFPmxYPaCjXEREMoq9SPRJwGnAWiDl7lfk3T8RuAF4ATgc+IW7P1TmrEB2hN7YUEcXsE0jdBERoIgRupk1AbcBF7j75cBhZnZi3mYXAU+7+zXAtcCN5Q6akT9C1xy6iEhaMSP0KcBb7r45WH4GmA48kbPNGiAZ3E4CLw70pK2tTTQ01JcQNe3d9ZuA9AgdYGRTI8lkc8nPE6Zqy5OhXKVRrtJUay6o3mzlzlVMoY8FOnKWNwTrct0E/M7MbgI+Clw50JO2t3cXm7HfxzUGvwza12+kra1jRw+JVDLZXFV5MpSrNMpVmmrNBdWbbbC5dvRLoJg3RdcCuc/QEqzLdTdwh7vPAT4H3GdmY0qLWZzeo1yCEbrm0EVE0oop9CXABDMbHiwfBywwszFm1hKs2xN4N7jdDvQU+dwlSwXXLGpoyBy2qDl0EREoonTdvRs4F7jZzOYBL7v7E8BcYFaw2QXAeWZ2CfBT4BJ3fz+MwLlHuQD89qmV3HjfS3Rv+iCMlxMRiY2iDlt090XAorx1F+Xcfhp4urzR+pc5yqVxWPYN1WWr/s5/vNnO0QfmT+2LiOw8YndiUU/eHHrGO22dFUgjIlI9Ylfo/Y3QAd5eq0IXkZ1bDAs9/Wf+CH11W1cF0oiIVI8YFvr2I/TxyV1Yu24jXXpjVER2YjEs9PSfjTkj9MP3/zAAy99ZX4lIIiJVIXaFnhmZNzc19q6zvUYDsPztdRXJJCJSDWJX6AdPbGXO6ZOZdtT43nX77t5CfV2Cl954n209PSz563s88uyb/O19zauLyM6jqOPQq0l9XR2H7PMhRo0c1rtuRGMDx076CM/+9T2+cdNTbNmaPnv0d39cyccmfYQJH2mma9NW1ndtYeTwelpGNdK6y3BGDm9g67YU23p6SKXSV0FKJBIkSP9ZlwASmdvp5URd+v6M3NskEry7fhPr1m3sXZ/os0Fm3fYrc1f1fYUiJYIsiezjc59z3aatg/78nKFmy/9yM19/AujamqK9vat3OfcBO9qHhV+r72N790vwRIns6pz7t99fdY0NtHds7rM+kZ8tdz19v6+J4HWHNdQxbBAfQicyGLEr9IwRjQ1cNvNodh2V/kSCU/9hb15e8V90bvyAIw9Icti+H2LRC2+zZNkalixbU+G0srNqbKjje189ht0+NKrSUWQnENtCB9h7XEvv7XFjmrhh1j+wvmsLydEjATj+0N1Y8bf1rO/cwqgRDey6y3A2btnKhs4ttHduZvOWbTTU11FXF4zGSZ+4lEqlSAV/9qTSnx+TSkFPT6r3mqYA9HNzVFMjnV2byZd5WN+PEkv1uS9/+2JHpqkgY/C/frKlGDmykY0btwzquXe0PkVqh6P2zGfvkP/1BzdGjBzGxo1btlufXU71Wc7NnvuqqT5Zs/t1++dN9XmN7R4T/N/w4cPYtPmD3qOqcuP0m63Pc6YX1rZvZPX7Xaxt36hCl0jEutDzNQ6r7y1zgLq6BPuPHx1phlr7qM6w1XKux/70n/zq/71R8JeiSLnF7k1RkbjI/Ksvf5QvEhYVukhIMm+S9qjPJSIqdJGQ1AVDdI3QJSoqdJGQJHrfaFehSzRU6CIhyUy5qM8lKip0kZAk9KaoREyFLhKSOo3QJWJFHYduZicBpwFrgZS7X5F3fwL412Bxb2C0u3+tjDlFYidz4pPm0CUqAxa6mTUBtwEHu/tmM/uNmZ0YXCg640xgnbvfEzzmsHDiisSH5tAlasWM0KcAb7l75nz2Z4DpQG6hnwE8ZmazgXHAHWVNKRJDdcGEpkboEpViCn0skHsO9IZgXa4JQIu7f9/MDiBd7ge5+7ZCT9ra2kTDED+FLplsHtLjw6JcpanVXLu2pD+ff5ddhpf1a6zV/RWmas1W7lzFFPpaIPdVW4J1uTYAfwJw99fNrAXYE3iz0JMO9qNcM2r5M0DCoFylKUeujs5NAGzYsKlsX2Mt76+wVGu2weba0S+BYo5yWQJMMLPhwfJxwAIzGxMUN6SnX/YBCNbVA++VnFSkhmSPctGUi0RjwEJ3927gXOBmM5sHvBy8IToXmBVsdi1wuJldAvwQOMvdN4WUWSQW9FkuErWiDlt090XAorx1F+XcXg+cU95oIvGWOWxRI3SJik4sEgmJDluUqKnQRUKiwxYlaip0kZBohC5RU6GLhERXLJKoqdBFQpI9ykWFLtFQoYuEJPvxuZXNITsPFbpISBI6sUgipkIXCUnmh0t9LlFRoYuEJHORaM2hS1RU6CIh0an/EjUVukhIdE1RiZoKXSQkOrFIoqZCFwmJRugSNRW6SEjqNEKXiKnQRUKSGaHrKBeJigpdJCQJNEKXaKnQRUKi49Alaip0kZDoTVGJmgpdJCQ6bFGiVtQ1Rc3sJOA0YC2QcvcrCmx3BnAv0OzunWVLKRJD+jx0idqAI3QzawJuAy5w98uBw8zsxH62OwiYVPaEIjGlU/8lasWM0KcAb7n75mD5GWA68ERmg6D0LwLOAS4p5oVbW5toaKgvLW2eZLJ5SI8Pi3KVplZzfRAU+vDhDWX9Gmt1f4WpWrOVO1cxhT4W6MhZ3hCsy3UV8H1332JmRb1we3t3UdsVkkw209bWMfCGEVOu0tRyrvZ1GwHYuPGDsn2Ntby/wlKt2Qaba0e/BIp5U3QtkPsMLcE6AMxsT6AV+CczmxusnmNmR5ecVKSG1OnEIolYMSP0JcAEMxseTLscB/zEzMYAW939bWBmZmMz+wFwk94UlZ1dna5YJBEbcITu7t3AucDNZjYPeNndnwDmArMy25lZ0swuDRYvMrM9wggsEhc6bFGiVtRhi+6+CFiUt+6ivOU2YF7wn8hOT5/lIlHTiUUiIdEIXaKmQhcJiU79l6ip0EVCos9Dl6ip0EVCosMWJWoqdJGQaA5doqZCFwmJjnKRqKnQRUKiEbpETYUuEhJ9fK5ETYUuEpKETv2XiKnQRUKSnUOvbA7ZeajQRUKiD+eSqKnQRUKiKxZJ1FToIiFKJDRCl+io0EVCVJdI6LBFiYwKXSREGqFLlFToIiFKJBKaQ5fIqNBFQqQRukRJhS4SorpEQp/lIpEp6hJ0ZnYScBqwFki5+xV5918MjAPeBY4GLnP318qcVSR2EnpTVCI04AjdzJqA24AL3P1y4DAzOzFvs12AOe5+HfAb4PpyBxWJozpNuUiEihmhTwHecvfNwfIzwHTgicwG7v7dnO3rgM6yJRSJMY3QJUrFFPpYoCNneUOwbjtm1gicBXxjoCdtbW2ioaG+mIwFJZPNQ3p8WJSrNLWcq74+QV19oqxfYy3vr7BUa7Zy5yqm0NcCua/aEqzrIyjznwLfcfcVAz1pe3t3sRn7lUw209bWMfCGEVOu0tR6rlQKPvigp2xfY63vrzBUa7bB5trRL4FijnJZAkwws+HB8nHAAjMbY2Yt0DvPfjtwk7u/aGafLzmlSA3SYYsSpQFH6O7ebWbnAjebWRvwsrs/YWbXAX8HrgHuBQ4BJpoZwCjSb46K7NR06r9EqajDFt19EbAob91FObdPK3MukZpQl9A1RSU6OrFIJETpo1xU6BINFbpIiBIJfR66REeFLhIijdAlSip0kRDpxCKJkgpdJEQ69V+ipEIXCZE+D12ipEIXCZFG6BIlFbpIiDRClyip0EVCpFP/JUoqdJEQ6SgXiZIKXSREmkOXKKnQRUKkOXSJkgpdJESaQ5coqdBFQlSXSOjTFiUyKnSREOlNUYmSCl0kRHWJ9J+adpEoqNBFQpRIpBtdfS5RUKGLhCjoc82jSySKugSdmZ0EnAasBVLufkXe/SOAG4DVwP7ANe7+epmzisSORugSpQFH6GbWBNwGXODulwOHmdmJeZt9E/hPd/8B8EPgznIHFYmjhObQJULFjNCnAG+5++Zg+RlgOvBEzjbTgUsA3P0VM5tsZi3uvqGsaUVipi5o9Nnz/wiJoT9ftV4BqVpzQXVmO+moPTn3i4eX/XmLKfSxQEfO8oZgXTHbFCz01tYmGhrqi4zZv2SyeUiPD4tylaaWc31yykQ2btlGddWJVNrE8aOB8v/dL6bQ1wK5r9oSrCt1mz7a27uLyVdQMtlMW1vHwBtGTLlKU+u5DhzfwrfPOLIMidJqfX+FoZqzDSbXjn4JFHOUyxJggpkND5aPAxaY2RgzawnWLSA9NYOZHQr8RdMtIiLRGrDQ3b0bOBe42czmAS+7+xPAXGBWsNl80qV/KfBvwNkh5RURkQKKOmzR3RcBi/LWXZRzeyPwjfJGExGRUujEIhGRGqFCFxGpESp0EZEaoUIXEakRKnQRkRqRqLZTYkVEZHA0QhcRqREqdBGRGqFCFxGpESp0EZEaoUIXEakRKnQRkRqhQhcRqRFFfdpiNRnogtURZ3kO2BQsbnP3E81sDHANsJL0BbMvcfc1IecYB8wDJrv7McG6ghfuNrMzgSOAbcAKd7894mwzga+T3Xd3uvvPo8pmZvsGmf4MjAf+y92/v6PvnZl9i/SFW1qBx939oQhzXQ5My9n0quATUCPJFbxOHfAw8CegEdgX+Bowksrus0K5Lqby+2xkkOtxd78wip/JWBV6zgWrD3b3zWb2GzM7Mfh89kp4LLhwdq6rgT+4+6/M7NOkv4EzQs5xPPAgkHuRwsyFu68LLjpyJzDVzMYDFwJHuHvKzF4wsyfdfXmE2QC+5O5v5q6IMNsY4Jfu/mDwuv9hZguA/0U/3zszOxb4uLufYmYNwKtmttjd10eUC3eflr9xhLkylrj7vOC1HyQ9sJpKZfdZoVzVsM/mAf+esxz6z2TcplwKXbC6Ug41s4vN7HIzy+SYTvoqTxBRPne/n77XdO2Tw91fASYHV5g6GXjR3TOnCC8BPhVxNoDzzOxCM7ssGBkTVTZ3fyFTmoE6oIvC37tTye7LrcCrwAkR5sLMvhPsr4uDgU1kuYLn78kpzQbS/4JwKr/PCuWq6D4zsxmk98eqnNWh/0zGrdCLuWB1lK5192uBK4FLzOy/0TfjBqA1+IsWtUL7qhr24WLS++4GYCnw62B95NnM7HPAQnd/jcLfu0rn+jXwo2B/dQC3BJtVItfJwCPAI+6+lCrZZ/3kqtg+M7NJwEHu/tu8u0L/mYxboZd8MeowufvzwZ/bgD8CH6dvxhagPRgJRK3Qvqr4PnT3Ve7eFiw+CZxgZvVRZzOzj5P+nl0QrCr0vatoLndf5u5dwd1PAp/oJ2/ouYIsC939k8BEM5tFleyz/FwV3mefAzaZ2VzSU44fNbNv7uC1y5YpboXe7wWrKxHEzA40s9xrp+4PrCDngtlUMB+FL9y9EDjKzBLBdlOAR6MMZmY/yPlXy/7Am8EvxciyBVNkJwPnA+PMbAqFv3e5+3IYcBDwVFS5zOz6nE0yf8+izjUpZ1oR0lMJ+1DhfVYoVyX3mbtf5e7fd/drgKeB5939R0TwMxm7T1s0s/8OfAFoAz6o1FEuZrY7cCvpNz1agGHAHGA0cC3wFul33OdGcJTLCcBXgE8CPwVuDO66AXgX2A+4Ou8d9aNJv6P+eshHufSX7V+AQ0j/8B0KzHf356LKZmZHkZ72WRqsGgX8GHiIAt+74MiI1uC/R0M6YqNQLgOaSI/aDgUuy/lehp4reJ19getJH4GTKcLZwBYqu88K5Tqfyu+zz5O+1nIj6e/jA4T8Mxm7QhcRkf7FbcpFREQKUKGLiNQIFbqISI1QoYuI1AgVuohIjVChi4jUCBW6iEiN+P+bZ1hR40gNCwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.LongTensor([0.2234])\n",
        "y = torch.LongTensor([1])\n",
        "print(torch.nn.CrossEntropyLoss(x,y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs4OYWCAE98S",
        "outputId": "cd84b727-c642-467f-e942-9b85db223b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossEntropyLoss()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(y_train_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_F2liekBKfu",
        "outputId": "aa43ba5a-e8ec-4f8f-84a2-1ca25eb52ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([98750, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd jolproject/Adv-ALSTM/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYV2DC7mMCSO",
        "outputId": "5b83c4a0-33c0-4b92-b770-36b4293c7d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/jolproject/Adv-ALSTM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "fnames = [fname for fname in os.listdir(\"./data/stocknet-dataset/price/ourpped\") if\n",
        "              os.path.isfile(os.path.join(\"./data/stocknet-dataset/price/ourpped\", fname))]\n",
        "\n",
        "\n",
        "# fnames = [fname for fname in os.listdir(\"./data/kdd17/ourpped/\") if\n",
        "#                os.path.isfile(os.path.join(\"./data/kdd17/ourpped/\", fname))]\n",
        "\n",
        "\n",
        "print(len(fnames))\n",
        "\n",
        "for fname in fnames:\n",
        "  temp = \"./data/stocknet-dataset/price/ourpped/\" + fname\n",
        "  data = pd.read_csv(temp)\n",
        "  print(fname,len(data))\n",
        "  # for i in range(len(data)-1):\n",
        "  #   if data.iloc[i,3]>0:\n",
        "  #     data.iloc[i,11]=1\n",
        "  #   else:\n",
        "  #     data.iloc[i,11]=0\n",
        "  \n",
        "  \n",
        "  #data.to_csv(temp,index = False)\n",
        "  "
      ],
      "metadata": {
        "id": "dUgvrl-xDGx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fb6378-3696-4c1e-91f6-96c080d1fa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87\n",
            "AAPL.csv 651\n",
            "ABB.csv 651\n",
            "ABBV.csv 651\n",
            "AEP.csv 651\n",
            "AGFS.csv 651\n",
            "AMGN.csv 651\n",
            "AMZN.csv 651\n",
            "BA.csv 651\n",
            "BABA.csv 651\n",
            "BAC.csv 651\n",
            "BBL.csv 651\n",
            "BCH.csv 651\n",
            "BHP.csv 651\n",
            "BP.csv 651\n",
            "BRK-A.csv 651\n",
            "BSAC.csv 651\n",
            "BUD.csv 651\n",
            "C.csv 651\n",
            "CAT.csv 651\n",
            "CELG.csv 651\n",
            "CHL.csv 651\n",
            "CHTR.csv 651\n",
            "CMCSA.csv 651\n",
            "CODI.csv 651\n",
            "CSCO.csv 651\n",
            "CVX.csv 651\n",
            "D.csv 651\n",
            "DHR.csv 651\n",
            "DIS.csv 651\n",
            "DUK.csv 651\n",
            "EXC.csv 651\n",
            "FB.csv 651\n",
            "GD.csv 651\n",
            "GE.csv 651\n",
            "GOOG.csv 651\n",
            "HD.csv 651\n",
            "HON.csv 651\n",
            "HRG.csv 651\n",
            "HSBC.csv 651\n",
            "IEP.csv 651\n",
            "INTC.csv 651\n",
            "JNJ.csv 651\n",
            "JPM.csv 651\n",
            "KO.csv 651\n",
            "LMT.csv 651\n",
            "MA.csv 651\n",
            "MCD.csv 651\n",
            "MDT.csv 651\n",
            "MMM.csv 651\n",
            "MO.csv 651\n",
            "MRK.csv 651\n",
            "MSFT.csv 651\n",
            "NEE.csv 651\n",
            "NGG.csv 651\n",
            "NVS.csv 651\n",
            "ORCL.csv 651\n",
            "PCG.csv 651\n",
            "PCLN.csv 651\n",
            "PEP.csv 651\n",
            "PFE.csv 651\n",
            "PG.csv 651\n",
            "PICO.csv 651\n",
            "PM.csv 651\n",
            "PPL.csv 651\n",
            "PTR.csv 651\n",
            "RDS-B.csv 651\n",
            "REX.csv 651\n",
            "SLB.csv 651\n",
            "SNP.csv 651\n",
            "SNY.csv 651\n",
            "SO.csv 651\n",
            "SPLP.csv 651\n",
            "SRE.csv 651\n",
            "T.csv 651\n",
            "TM.csv 651\n",
            "TOT.csv 651\n",
            "TSM.csv 651\n",
            "UL.csv 651\n",
            "UN.csv 651\n",
            "UNH.csv 651\n",
            "UPS.csv 651\n",
            "UTX.csv 651\n",
            "V.csv 651\n",
            "VZ.csv 651\n",
            "WFC.csv 651\n",
            "WMT.csv 651\n",
            "XOM.csv 651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=set()\n",
        "for i in range(len(data)):\n",
        "  t.add(data.iloc[i][11])\n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnNdOsdBF6Q0",
        "outputId": "a7e59959-2c85-4c96-89f6-66337703677c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0, 1.0, -123321.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k=set()\n",
        "for fname in fnames:\n",
        "  temp = \"./data/kdd17/ourpped/\" + fname\n",
        "  print(temp)\n",
        "  data = pd.read_csv(temp,index_col=0)\n",
        "  print(data)\n",
        "  print(data.iloc[:,11])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAexHLFUjG_W",
        "outputId": "e79299fd-eb30-4869-916b-8fa8f8d4f2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/kdd17/ourpped/AAPL.csv\n",
            "            -123321.000000  -123321.000000.1  -123321.000000.2  \\\n",
            "Unnamed: 0                                                       \n",
            "0           -123321.000000    -123321.000000    -123321.000000   \n",
            "1           -123321.000000    -123321.000000    -123321.000000   \n",
            "2           -123321.000000    -123321.000000    -123321.000000   \n",
            "3           -123321.000000    -123321.000000    -123321.000000   \n",
            "4           -123321.000000    -123321.000000    -123321.000000   \n",
            "...                    ...               ...               ...   \n",
            "2512             -0.798147          0.000000         -0.798147   \n",
            "2513             -0.631080          0.460516         -0.656664   \n",
            "2514              0.650904          1.079132         -0.479621   \n",
            "2515             -0.239875          0.325536         -0.282705   \n",
            "2516              0.716631          1.191501         -0.336729   \n",
            "\n",
            "            -123321.000000.3  -123321.000000.4  -123321.000000.5  \\\n",
            "Unnamed: 0                                                         \n",
            "0             -123321.000000    -123321.000000    -123321.000000   \n",
            "1             -123321.000000    -123321.000000    -123321.000000   \n",
            "2             -123321.000000    -123321.000000    -123321.000000   \n",
            "3             -123321.000000    -123321.000000    -123321.000000   \n",
            "4             -123321.000000    -123321.000000    -123321.000000   \n",
            "...                      ...               ...               ...   \n",
            "2512                0.197778          0.197778          0.147615   \n",
            "2513                0.635088          0.635088         -0.378648   \n",
            "2514               -0.426403         -0.426403          0.015415   \n",
            "2515               -0.025693         -0.025693         -0.015422   \n",
            "2516               -0.779579         -0.779579          0.689001   \n",
            "\n",
            "            -123321.000000.6  -123321.000000.7  -123321.000000.8  \\\n",
            "Unnamed: 0                                                         \n",
            "0             -123321.000000    -123321.000000    -123321.000000   \n",
            "1             -123321.000000    -123321.000000    -123321.000000   \n",
            "2             -123321.000000    -123321.000000    -123321.000000   \n",
            "3             -123321.000000    -123321.000000    -123321.000000   \n",
            "4             -123321.000000    -123321.000000    -123321.000000   \n",
            "...                      ...               ...               ...   \n",
            "2512               -0.538102         -1.871493         -2.676362   \n",
            "2513               -0.828076         -2.027405         -3.047930   \n",
            "2514               -0.268930         -1.219026         -2.405791   \n",
            "2515               -0.111371         -0.868102         -2.114712   \n",
            "2516                0.673459          0.123755         -1.072353   \n",
            "\n",
            "            -123321.000000.9  -123321.000000.10  -123321.000000.11  \\\n",
            "Unnamed: 0                                                           \n",
            "0             -123321.000000     -123321.000000          -123321.0   \n",
            "1             -123321.000000     -123321.000000          -123321.0   \n",
            "2             -123321.000000     -123321.000000          -123321.0   \n",
            "3             -123321.000000     -123321.000000          -123321.0   \n",
            "4             -123321.000000     -123321.000000          -123321.0   \n",
            "...                      ...                ...                ...   \n",
            "2512               -3.033296          -3.712379                1.0   \n",
            "2513               -3.399626          -4.069022                1.0   \n",
            "2514               -2.813636          -3.342756                0.0   \n",
            "2515               -2.619723          -3.043208                0.0   \n",
            "2516               -1.696080          -2.113624                0.0   \n",
            "\n",
            "            -123321.000000.12  \n",
            "Unnamed: 0                     \n",
            "0              -123321.000000  \n",
            "1              -123321.000000  \n",
            "2              -123321.000000  \n",
            "3              -123321.000000  \n",
            "4              -123321.000000  \n",
            "...                       ...  \n",
            "2512               116.519997  \n",
            "2513               117.260002  \n",
            "2514               116.760002  \n",
            "2515               116.730003  \n",
            "2516               115.820000  \n",
            "\n",
            "[2517 rows x 13 columns]\n",
            "Unnamed: 0\n",
            "0      -123321.0\n",
            "1      -123321.0\n",
            "2      -123321.0\n",
            "3      -123321.0\n",
            "4      -123321.0\n",
            "          ...   \n",
            "2512         1.0\n",
            "2513         1.0\n",
            "2514         0.0\n",
            "2515         0.0\n",
            "2516         0.0\n",
            "Name: -123321.000000.11, Length: 2517, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if 'acl18' in args.path:\n",
        "#         tra_date = '2014-01-02'\n",
        "#         val_date = '2015-08-03'\n",
        "#         tes_date = '2015-10-01'\n",
        "#     elif 'kdd17' in args.path:\n",
        "#         tra_date = '2007-01-03'\n",
        "#         val_date = '2015-01-02'\n",
        "#         tes_date = '2016-01-04'\n",
        "\n",
        "#s&p 500  \n",
        "# tra_date = '2007-01-03'\n",
        "# val_date = '2015-01-02'\n",
        "# tes_date = '2016-01-04'\n",
        "\n",
        "\n",
        "tra_date = '2014-01-02'\n",
        "val_date = '2015-08-03'\n",
        "tes_date = '2015-10-01'\n",
        "#end date 2016-12-31\n",
        "import pandas_datareader.data as web\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "sp500 = web.DataReader('^GSPC', data_source='yahoo', start='6/3/2013', end='12/31/2015')"
      ],
      "metadata": {
        "id": "h_ZgT_-BkTFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyJVE-WxCkv3",
        "outputId": "721d5338-fb50-4b18-b0f0-0b0ed08261d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 504 entries, 2014-01-02 to 2015-12-31\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   High       504 non-null    float64\n",
            " 1   Low        504 non-null    float64\n",
            " 2   Open       504 non-null    float64\n",
            " 3   Close      504 non-null    float64\n",
            " 4   Volume     504 non-null    int64  \n",
            " 5   Adj Close  504 non-null    float64\n",
            "dtypes: float64(5), int64(1)\n",
            "memory usage: 27.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "-9lS2cKwDEJu",
        "outputId": "bcfd37e5-7564-423a-d701-5f6a4ba16aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   High          Low         Open        Close      Volume  \\\n",
              "Date                                                                         \n",
              "2013-06-03  1640.420044  1622.719971  1631.709961  1640.420044  3952070000   \n",
              "2013-06-04  1646.530029  1623.619995  1640.729980  1631.380005  3653840000   \n",
              "2013-06-05  1629.310059  1607.089966  1629.050049  1608.900024  3632350000   \n",
              "2013-06-06  1622.560059  1598.229980  1609.290039  1622.560059  3547380000   \n",
              "2013-06-07  1644.400024  1625.270020  1625.270020  1643.380005  3371990000   \n",
              "...                 ...          ...          ...          ...         ...   \n",
              "2015-12-24  2067.360107  2058.729980  2063.520020  2060.989990  1411860000   \n",
              "2015-12-28  2057.770020  2044.199951  2057.770020  2056.500000  2492510000   \n",
              "2015-12-29  2081.560059  2060.540039  2060.540039  2078.360107  2542000000   \n",
              "2015-12-30  2077.340088  2061.969971  2077.340088  2063.360107  2367430000   \n",
              "2015-12-31  2062.540039  2043.619995  2060.590088  2043.939941  2655330000   \n",
              "\n",
              "              Adj Close  \n",
              "Date                     \n",
              "2013-06-03  1640.420044  \n",
              "2013-06-04  1631.380005  \n",
              "2013-06-05  1608.900024  \n",
              "2013-06-06  1622.560059  \n",
              "2013-06-07  1643.380005  \n",
              "...                 ...  \n",
              "2015-12-24  2060.989990  \n",
              "2015-12-28  2056.500000  \n",
              "2015-12-29  2078.360107  \n",
              "2015-12-30  2063.360107  \n",
              "2015-12-31  2043.939941  \n",
              "\n",
              "[652 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ff54016-34c6-4014-aa34-7b9fb2f702d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2013-06-03</th>\n",
              "      <td>1640.420044</td>\n",
              "      <td>1622.719971</td>\n",
              "      <td>1631.709961</td>\n",
              "      <td>1640.420044</td>\n",
              "      <td>3952070000</td>\n",
              "      <td>1640.420044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-06-04</th>\n",
              "      <td>1646.530029</td>\n",
              "      <td>1623.619995</td>\n",
              "      <td>1640.729980</td>\n",
              "      <td>1631.380005</td>\n",
              "      <td>3653840000</td>\n",
              "      <td>1631.380005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-06-05</th>\n",
              "      <td>1629.310059</td>\n",
              "      <td>1607.089966</td>\n",
              "      <td>1629.050049</td>\n",
              "      <td>1608.900024</td>\n",
              "      <td>3632350000</td>\n",
              "      <td>1608.900024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-06-06</th>\n",
              "      <td>1622.560059</td>\n",
              "      <td>1598.229980</td>\n",
              "      <td>1609.290039</td>\n",
              "      <td>1622.560059</td>\n",
              "      <td>3547380000</td>\n",
              "      <td>1622.560059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2013-06-07</th>\n",
              "      <td>1644.400024</td>\n",
              "      <td>1625.270020</td>\n",
              "      <td>1625.270020</td>\n",
              "      <td>1643.380005</td>\n",
              "      <td>3371990000</td>\n",
              "      <td>1643.380005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-12-24</th>\n",
              "      <td>2067.360107</td>\n",
              "      <td>2058.729980</td>\n",
              "      <td>2063.520020</td>\n",
              "      <td>2060.989990</td>\n",
              "      <td>1411860000</td>\n",
              "      <td>2060.989990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-12-28</th>\n",
              "      <td>2057.770020</td>\n",
              "      <td>2044.199951</td>\n",
              "      <td>2057.770020</td>\n",
              "      <td>2056.500000</td>\n",
              "      <td>2492510000</td>\n",
              "      <td>2056.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-12-29</th>\n",
              "      <td>2081.560059</td>\n",
              "      <td>2060.540039</td>\n",
              "      <td>2060.540039</td>\n",
              "      <td>2078.360107</td>\n",
              "      <td>2542000000</td>\n",
              "      <td>2078.360107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-12-30</th>\n",
              "      <td>2077.340088</td>\n",
              "      <td>2061.969971</td>\n",
              "      <td>2077.340088</td>\n",
              "      <td>2063.360107</td>\n",
              "      <td>2367430000</td>\n",
              "      <td>2063.360107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-12-31</th>\n",
              "      <td>2062.540039</td>\n",
              "      <td>2043.619995</td>\n",
              "      <td>2060.590088</td>\n",
              "      <td>2043.939941</td>\n",
              "      <td>2655330000</td>\n",
              "      <td>2043.939941</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>652 rows  6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ff54016-34c6-4014-aa34-7b9fb2f702d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ff54016-34c6-4014-aa34-7b9fb2f702d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ff54016-34c6-4014-aa34-7b9fb2f702d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp500['zopen']=sp500['Open']/sp500['Close'] -1\n",
        "sp500['zhigh']=sp500['High']/sp500['Close'] -1\n",
        "sp500['zlow']=sp500['Low']/sp500['Close']-1"
      ],
      "metadata": {
        "id": "qwhnzFVmEEB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp500['zclose']=-123320\n",
        "sp500['zadjclose']=-123320\n",
        "sp500['z5']=-123320\n",
        "sp500['z10']=-123320\n",
        "sp500['z15']=-123320\n",
        "sp500['z20']=-123320\n",
        "sp500['z25']=-123320\n",
        "sp500['z30']=-123320"
      ],
      "metadata": {
        "id": "Dj1oL0XWFjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sp500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBxLnBJOG7Dh",
        "outputId": "be7e78c4-a128-48b6-c05d-49cb8fc5b435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "652"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,len(sp500)):\n",
        "  sp500.iloc[i,9]=sp500.iloc[i,3]/sp500.iloc[i-1,3] -1"
      ],
      "metadata": {
        "id": "7hJHdOSlHC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,len(sp500)):\n",
        "  sp500.iloc[i,10]=sp500.iloc[i,5]/sp500.iloc[i-1,5] -1"
      ],
      "metadata": {
        "id": "eMH_yBeVH8vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(5):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,11]=mysum/(5*sp500.iloc[i,5]) -1"
      ],
      "metadata": {
        "id": "k57pt9OQIYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(10):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,12]=mysum/(10*sp500.iloc[i,5]) -1\n",
        "\n",
        "for i in range(14,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(15):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,13]=mysum/(15*sp500.iloc[i,5]) -1\n",
        "\n",
        "for i in range(19,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(20):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,14]=mysum/(20*sp500.iloc[i,5]) -1\n",
        "\n",
        "for i in range(24,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(25):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,15]=mysum/(25*sp500.iloc[i,5]) -1\n",
        "\n",
        "for i in range(29,len(sp500)):\n",
        "  mysum=0\n",
        "  for j in range(30):\n",
        "    mysum=mysum+sp500.iloc[i-j,5]\n",
        "  sp500.iloc[i,16]=mysum/(30*sp500.iloc[i,5]) -1"
      ],
      "metadata": {
        "id": "0twZKeu5Jk0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sp500 = sp500.iloc[:,6:]"
      ],
      "metadata": {
        "id": "7hT30NinKWcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sp500.iloc[28]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTNdjo5aKmbk",
        "outputId": "cbb10d4f-bb28-415a-9a5f-38cdf5da4c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "zopen            -0.002934\n",
              "zhigh             0.000000\n",
              "zlow             -0.004678\n",
              "zclose            0.003086\n",
              "zadjclose         0.003086\n",
              "z5               -0.011944\n",
              "z10              -0.024918\n",
              "z15              -0.033711\n",
              "z20              -0.033206\n",
              "z25              -0.032273\n",
              "z30         -123320.000000\n",
              "Name: 2013-07-12 00:00:00, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sp500.iloc[:29,:]=-123320"
      ],
      "metadata": {
        "id": "nDcKfUx_K5V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sp500.iloc[29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EfmXoVgLCD7",
        "outputId": "96b33b06-1206-4956-931e-ef93f4639bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "zopen       -0.001730\n",
              "zhigh        0.001195\n",
              "zlow        -0.002740\n",
              "zclose       0.001375\n",
              "zadjclose    0.001375\n",
              "z5          -0.008303\n",
              "z10         -0.021727\n",
              "z15         -0.031469\n",
              "z20         -0.032876\n",
              "z25         -0.032671\n",
              "z30         -0.032493\n",
              "Name: 2013-07-15 00:00:00, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sp500.to_csv('sp500_acl18.csv', header=False, index=False)"
      ],
      "metadata": {
        "id": "0t593J41LuZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJE_DE-RLHk4",
        "outputId": "efed44ec-19f0-43ce-d932-09fef73c4fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade pandas-datareader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "83jFQicYC4YE",
        "outputId": "16b708fa-9b59-48d1-c8bc-004e54d3c5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas-datareader in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Collecting pandas-datareader\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
            "\u001b[K     || 109 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (1.3.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pandas-datareader) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->pandas-datareader) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23->pandas-datareader) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pandas-datareader) (3.0.4)\n",
            "Installing collected packages: pandas-datareader\n",
            "  Attempting uninstall: pandas-datareader\n",
            "    Found existing installation: pandas-datareader 0.9.0\n",
            "    Uninstalling pandas-datareader-0.9.0:\n",
            "      Successfully uninstalled pandas-datareader-0.9.0\n",
            "Successfully installed pandas-datareader-0.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas_datareader"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E5qApgKoC4pv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}